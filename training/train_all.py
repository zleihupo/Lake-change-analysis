# -*- coding: utf-8 -*-
"""train_all.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WA7Iz9zK4OdhToo9t8SBCQZr9NiJ1qNe
"""

from google.colab import drive
# Mount Google Drive for dataset access
drive.mount('/content/drive')

import os
import shutil
import random

# Define dataset paths and desired split ratios
image_dir = '/content/drive/My Drive/train/img'
mask_dir = '/content/drive/My Drive/train/mask'
output_base = '/content/drive/My Drive/dataset/'
splits = ['train', 'val', 'test']
split_ratio = {'train': 0.7, 'val': 0.1, 'test': 0.2}

# Create output folders for each split and type (images/masks)
for split in splits:
    os.makedirs(os.path.join(output_base, split, 'images'), exist_ok=True)
    os.makedirs(os.path.join(output_base, split, 'masks'), exist_ok=True)

# Get all image filenames and randomly shuffle them
all_files = sorted([f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png'))])
random.shuffle(all_files)
total = len(all_files)

# Split filenames according to the specified ratio
train_end = int(split_ratio['train'] * total)
val_end = train_end + int(split_ratio['val'] * total)
split_files = {
    'train': all_files[:train_end],
    'val': all_files[train_end:val_end],
    'test': all_files[val_end:]
}

# Copy image and corresponding mask files into respective folders
for split, files in split_files.items():
    for f in files:
        img_src = os.path.join(image_dir, f)
        mask_name = f.replace('.jpg', '.png').replace('.jpeg', '.png')
        mask_src = os.path.join(mask_dir, mask_name)
        shutil.copy(img_src, os.path.join(output_base, split, 'images', f))
        shutil.copy(mask_src, os.path.join(output_base, split, 'masks', mask_name))

# Print summary of split counts
print("Dataset split completed, total:")
for k, v in split_files.items():
    print(f"{k}: {len(v)} images")

# Step 1: Train U-Net model (train_unet.py)
# This section implements a U-Net model with dropout for regularization, metrics for evaluation, and training callbacks.

import os, random, glob
import numpy as np
import tensorflow as tf
import cv2
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, Dropout, concatenate
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from sklearn.model_selection import train_test_split
from tqdm import tqdm

# Set fixed random seeds to ensure reproducible training results
SEED = 7
os.environ["PYTHONHASHSEED"] = str(SEED)
random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)

# Define image and mask directories for training and validation
train_img_path = '/content/drive/My Drive/dataset/train/images/'
train_mask_path = '/content/drive/My Drive/dataset/train/masks/'
val_img_path   = '/content/drive/My Drive/dataset/val/images/'
val_mask_path  = '/content/drive/My Drive/dataset/val/masks/'

IMG_SIZE = (256, 256)

# Load and preprocess image-mask pairs
# Convert images to RGB, resize, normalize; binarize masks

def load_data(img_dir, mask_dir, img_size=(256, 256)):
    images, masks = [], []
    img_files = sorted(glob.glob(os.path.join(img_dir, '*')))
    for img_path in tqdm(img_files, desc=f'Loading {os.path.basename(os.path.normpath(img_dir))}'):
        fname = os.path.basename(img_path)
        mask_path = os.path.join(mask_dir, fname.replace('.jpg', '.png'))

        img = cv2.imread(img_path)
        if img is None:
            continue
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img = cv2.resize(img, img_size)
        img = (img / 255.0).astype(np.float32)

        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
        if mask is None:
            continue
        mask = cv2.resize(mask, img_size, interpolation=cv2.INTER_NEAREST)
        mask = (mask > 127).astype(np.float32)[..., None]  # shape (H, W, 1)

        images.append(img)
        masks.append(mask)

    return np.asarray(images, dtype=np.float32), np.asarray(masks, dtype=np.float32)

# Load datasets into memory
x_train, y_train = load_data(train_img_path, train_mask_path, IMG_SIZE)
x_val,   y_val   = load_data(val_img_path,   val_mask_path,   IMG_SIZE)

print('Train:', x_train.shape, y_train.shape, ' Val:', x_val.shape, y_val.shape)

# Define evaluation metrics: Intersection over Union (IoU) and Dice coefficient

def iou_metric(y_true, y_pred, smooth=1e-6):
    y_true = tf.cast(y_true, tf.float32)
    y_pred = tf.cast(y_pred, tf.float32)
    y_pred = tf.clip_by_value(y_pred, 0.0, 1.0)
    intersection = tf.reduce_sum(y_true * y_pred)
    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection
    return (intersection + smooth) / (union + smooth)

def dice_coef(y_true, y_pred, smooth=1e-6):
    y_true = tf.cast(y_true, tf.float32)
    y_pred = tf.cast(y_pred, tf.float32)
    y_pred = tf.clip_by_value(y_pred, 0.0, 1.0)
    intersection = tf.reduce_sum(y_true * y_pred)
    denom = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred)
    return (2.0 * intersection + smooth) / (denom + smooth)

# Define the U-Net architecture with skip connections and dropout in bottleneck

def unet_model(input_size=(256, 256, 3)):
    inputs = Input(input_size)

    def conv_block(x, filters, drop=False):
        x = Conv2D(filters, 3, activation='relu', padding='same')(x)
        x = Conv2D(filters, 3, activation='relu', padding='same')(x)
        if drop:
            x = Dropout(0.5)(x)
        return x

    c1 = conv_block(inputs, 64)
    p1 = MaxPooling2D()(c1)
    c2 = conv_block(p1, 128)
    p2 = MaxPooling2D()(c2)
    c3 = conv_block(p2, 256)
    p3 = MaxPooling2D()(c3)
    c4 = conv_block(p3, 512, drop=True)
    p4 = MaxPooling2D()(c4)
    c5 = conv_block(p4, 1024, drop=True)

    u6 = Conv2DTranspose(512, 2, strides=2, padding='same')(c5)
    u6 = concatenate([u6, c4])
    c6 = conv_block(u6, 512)

    u7 = Conv2DTranspose(256, 2, strides=2, padding='same')(c6)
    u7 = concatenate([u7, c3])
    c7 = conv_block(u7, 256)

    u8 = Conv2DTranspose(128, 2, strides=2, padding='same')(c7)
    u8 = concatenate([u8, c2])
    c8 = conv_block(u8, 128)

    u9 = Conv2DTranspose(64, 2, strides=2, padding='same')(c8)
    u9 = concatenate([u9, c1])
    c9 = conv_block(u9, 64)

    outputs = Conv2D(1, 1, activation='sigmoid')(c9)
    return Model(inputs, outputs)

# Instantiate and compile U-Net model using binary cross-entropy and custom metrics
model = unet_model(input_size=(IMG_SIZE[0], IMG_SIZE[1], 3))
model.compile(
    optimizer='adam',
    loss='binary_crossentropy',  # standard loss for binary segmentation
    metrics=['accuracy', iou_metric, dice_coef]
)

# Define callbacks: save best model, reduce learning rate on plateau, early stopping
ckpt_path = '/content/drive/My Drive/unet_model_best.h5'
callbacks = [
    ModelCheckpoint(ckpt_path, monitor='val_iou_metric', mode='max',
                    save_best_only=True, save_weights_only=False, verbose=1),
    ReduceLROnPlateau(monitor='val_iou_metric', mode='max',
                      factor=0.5, patience=5, min_lr=1e-6, verbose=1),
    EarlyStopping(monitor='val_iou_metric', mode='max',
                  patience=15, restore_best_weights=True, verbose=1)
]

# Train the model on training data with validation monitoring
history = model.fit(
    x_train, y_train,
    validation_data=(x_val, y_val),
    batch_size=8,
    epochs=100,
    callbacks=callbacks,
    shuffle=True,
    verbose=1
)

# Save the final model to disk
final_path = '/content/drive/My Drive/unet_model_final.h5'
model.save(final_path)
print(f"Best checkpoint: {ckpt_path}\nFinal model: {final_path}")


# Step 2: Train SegNet model (train_segnet.py)
# This section implements the SegNet model with batch normalization, dropout, and standard callbacks.

import os, random, glob
import numpy as np
import tensorflow as tf
import cv2
from tqdm import tqdm
from tensorflow.keras.layers import (Input, Conv2D, MaxPooling2D, UpSampling2D,
                                     Conv2DTranspose, BatchNormalization,
                                     Activation, Dropout)
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint

# Set seed values for reproducibility
SEED = 7
os.environ["PYTHONHASHSEED"] = str(SEED)
random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)

# Define paths for training and validation sets
train_img_path = '/content/drive/My Drive/dataset/train/images/'
train_mask_path = '/content/drive/My Drive/dataset/train/masks/'
val_img_path   = '/content/drive/My Drive/dataset/val/images/'
val_mask_path  = '/content/drive/My Drive/dataset/val/masks/'

IMG_SIZE = (256, 256)

# Load and preprocess images and masks
def load_data(img_dir, mask_dir, img_size=(256, 256)):
    images, masks = [], []
    img_files = sorted(glob.glob(os.path.join(img_dir, '*')))
    for img_path in tqdm(img_files, desc=f'Loading {os.path.basename(os.path.normpath(img_dir))}'):
        fname = os.path.basename(img_path)
        mask_path = os.path.join(mask_dir, fname.replace('.jpg', '.png'))

        img = cv2.imread(img_path)
        if img is None:
            continue
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img = cv2.resize(img, img_size)
        img = (img / 255.0).astype(np.float32)

        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
        if mask is None:
            continue
        mask = cv2.resize(mask, img_size, interpolation=cv2.INTER_NEAREST)
        mask = (mask > 127).astype(np.float32)[..., None]  # binary mask, shape (H, W, 1)

        images.append(img)
        masks.append(mask)

    return np.asarray(images, dtype=np.float32), np.asarray(masks, dtype=np.float32)

# Load train/val datasets
x_train, y_train = load_data(train_img_path, train_mask_path, IMG_SIZE)
x_val, y_val = load_data(val_img_path, val_mask_path, IMG_SIZE)

print('Train:', x_train.shape, y_train.shape, ' Val:', x_val.shape, y_val.shape)

# Define evaluation metrics for segmentation tasks
def iou_metric(y_true, y_pred, smooth=1e-6):
    y_true = tf.cast(y_true, tf.float32)
    y_pred = tf.clip_by_value(tf.cast(y_pred, tf.float32), 0.0, 1.0)
    inter = tf.reduce_sum(y_true * y_pred)
    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - inter
    return (inter + smooth) / (union + smooth)

def dice_coef(y_true, y_pred, smooth=1e-6):
    y_true = tf.cast(y_true, tf.float32)
    y_pred = tf.clip_by_value(tf.cast(y_pred, tf.float32), 0.0, 1.0)
    inter = tf.reduce_sum(y_true * y_pred)
    denom = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred)
    return (2.0 * inter + smooth) / (denom + smooth)

# Construct the SegNet architecture with batch normalization and dropout
def build_segnet(input_shape=(256, 256, 3)):
    inputs = Input(shape=input_shape)

    # Encoder blocks
    x = Conv2D(64, (3,3), padding='same')(inputs); x = BatchNormalization()(x); x = Activation('relu')(x)
    x = MaxPooling2D()(x)

    x = Conv2D(128, (3,3), padding='same')(x); x = BatchNormalization()(x); x = Activation('relu')(x)
    x = MaxPooling2D()(x)

    x = Conv2D(256, (3,3), padding='same')(x); x = BatchNormalization()(x); x = Activation('relu')(x)
    x = MaxPooling2D()(x)

    # Bottleneck with dropout for regularization
    x = Conv2D(512, (3,3), padding='same')(x); x = BatchNormalization()(x); x = Activation('relu')(x)
    x = Dropout(0.3)(x)

    # Decoder with upsampling and transposed convolution
    x = UpSampling2D()(x)
    x = Conv2DTranspose(256, (3,3), padding='same')(x); x = BatchNormalization()(x); x = Activation('relu')(x)

    x = UpSampling2D()(x)
    x = Conv2DTranspose(128, (3,3), padding='same')(x); x = BatchNormalization()(x); x = Activation('relu')(x)

    x = UpSampling2D()(x)
    x = Conv2DTranspose(64, (3,3), padding='same')(x); x = BatchNormalization()(x); x = Activation('relu')(x)

    outputs = Conv2DTranspose(1, (1,1), activation='sigmoid')(x)
    return Model(inputs, outputs)

# Initialize and compile SegNet model with binary cross-entropy and metrics
segnet = build_segnet(input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))
segnet.compile(optimizer='adam',
               loss='binary_crossentropy',
               metrics=['accuracy', iou_metric, dice_coef])

# Define callbacks for training
ckpt_path  = '/content/drive/My Drive/segnet_model_best.h5'
final_path = '/content/drive/My Drive/segnet_model_final.h5'
callbacks = [
    ModelCheckpoint(ckpt_path, monitor='val_iou_metric', mode='max',
                    save_best_only=True, save_weights_only=False, verbose=1),
    ReduceLROnPlateau(monitor='val_iou_metric', mode='max',
                      factor=0.5, patience=5, min_lr=1e-6, verbose=1),
    EarlyStopping(monitor='val_iou_metric', mode='max',
                  patience=15, restore_best_weights=True, verbose=1),
]

# Train the SegNet model on training data
history = segnet.fit(
    x_train, y_train,
    validation_data=(x_val, y_val),
    batch_size=8,
    epochs=100,
    callbacks=callbacks,
    shuffle=True,
    verbose=1
)

# Save the final SegNet model to disk
segnet.save(final_path)
print(f"Best checkpoint: {ckpt_path}\nFinal model: {final_path}")

# Step 3: Train FCN model (train_fcn.py)
# Implements FCN-8s model using a VGG16 backbone with skip connections and staged training.

import os, random, glob
import numpy as np
import tensorflow as tf
import cv2
from tqdm import tqdm
from tensorflow.keras.applications import VGG16
from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, Add, Activation
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint

# Set seed values for reproducibility
SEED = 7
os.environ["PYTHONHASHSEED"] = str(SEED)
random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)

# Define data paths for FCN training
train_img_path = '/content/drive/My Drive/dataset/train/images/'
train_mask_path = '/content/drive/My Drive/dataset/train/masks/'
val_img_path   = '/content/drive/My Drive/dataset/val/images/'
val_mask_path  = '/content/drive/My Drive/dataset/val/masks/'

IMG_SIZE = (256, 256)

# Load and preprocess training and validation images/masks
def load_data(img_dir, mask_dir, img_size=(256, 256)):
    patterns = ["*.jpg", "*.jpeg", "*.png", "*.tif", "*.tiff", "*.bmp"]
    img_files = []
    for pat in patterns:
        img_files.extend(glob.glob(os.path.join(img_dir, pat)))
    img_files = sorted(img_files)

    if len(img_files) == 0:
        raise FileNotFoundError(f"No images found in: {img_dir}")

    images, masks = [], []

    for img_path in tqdm(img_files, desc=f'Loading {os.path.basename(os.path.normpath(img_dir))}'):
        fname = os.path.basename(img_path)
        stem, _ = os.path.splitext(fname)
        mpath_candidates = [
            os.path.join(mask_dir, stem + ".png"),
            os.path.join(mask_dir, stem + ".jpg"),
            os.path.join(mask_dir, stem + ".tif"),
        ]
        mask_path = next((p for p in mpath_candidates if os.path.exists(p)), None)
        if mask_path is None:
            raise FileNotFoundError(f"Mask not found for image: {fname} in {mask_dir}")

        img = cv2.imread(img_path)
        if img is None:
            raise ValueError(f"Failed to read image: {img_path}")
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img = cv2.resize(img, img_size)
        img = (img / 255.0).astype(np.float32)

        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
        if mask is None:
            raise ValueError(f"Failed to read mask: {mask_path}")
        mask = cv2.resize(mask, img_size, interpolation=cv2.INTER_NEAREST)
        mask = (mask > 127).astype(np.float32)[..., None]  # (H, W, 1)

        images.append(img)
        masks.append(mask)

    X = np.asarray(images, dtype=np.float32)
    y = np.asarray(masks, dtype=np.float32)

    if X.size == 0 or y.size == 0:
        raise RuntimeError("Loaded empty arrays â€” check img/mask directories and names.")

    return X, y

# Load FCN training and validation sets
x_train, y_train = load_data(train_img_path, train_mask_path, IMG_SIZE)
x_val, y_val = load_data(val_img_path, val_mask_path, IMG_SIZE)

print('Train:', x_train.shape, y_train.shape, ' Val:', x_val.shape, y_val.shape)

# Define evaluation metrics: IoU and Dice

def iou_metric(y_true, y_pred, smooth=1e-6):
    y_true = tf.cast(y_true, tf.float32)
    y_pred = tf.clip_by_value(tf.cast(y_pred, tf.float32), 0.0, 1.0)
    inter = tf.reduce_sum(y_true * y_pred)
    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - inter
    return (inter + smooth) / (union + smooth)

def dice_coef(y_true, y_pred, smooth=1e-6):
    y_true = tf.cast(y_true, tf.float32)
    y_pred = tf.clip_by_value(tf.cast(y_pred, tf.float32), 0.0, 1.0)
    inter = tf.reduce_sum(y_true * y_pred)
    denom = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred)
    return (2.0 * inter + smooth) / (denom + smooth)

# Build FCN-8s architecture based on VGG16 backbone
def build_fcn(input_shape=(256, 256, 3)):
    vgg = VGG16(include_top=False, weights='imagenet', input_shape=input_shape)

    # Extract feature maps from VGG
    f3 = vgg.get_layer('block3_pool').output   # 1/8 resolution
    f4 = vgg.get_layer('block4_pool').output   # 1/16 resolution
    f5 = vgg.get_layer('block5_pool').output   # 1/32 resolution

    # Replace classifier with convolutional layers
    o = Conv2D(512, (7, 7), padding='same', activation='relu')(f5)
    o = Conv2D(512, (1, 1), padding='same', activation='relu')(o)
    o = Conv2D(1,   (1, 1), padding='same', activation=None)(o)

    # Upsample and add skip connections
    o = Conv2DTranspose(1, kernel_size=(4, 4), strides=(2, 2), padding='same', activation=None)(o)
    o2 = Conv2D(1, (1, 1), padding='same', activation=None)(f4)
    o = Add()([o, o2])

    o = Conv2DTranspose(1, kernel_size=(4, 4), strides=(2, 2), padding='same', activation=None)(o)
    o3 = Conv2D(1, (1, 1), padding='same', activation=None)(f3)
    o = Add()([o, o3])

    o = Conv2DTranspose(1, kernel_size=(8, 8), strides=(8, 8), padding='same', activation=None)(o)
    o = Activation('sigmoid')(o)

    model = Model(inputs=vgg.input, outputs=o)
    return model

# Instantiate FCN model and freeze encoder for warm-up training
fcn = build_fcn(input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))
for layer in fcn.layers:
    if layer.name.startswith('block'):
        layer.trainable = False

fcn.compile(optimizer='adam',
            loss='binary_crossentropy',
            metrics=['accuracy', iou_metric, dice_coef])

# Define callbacks for FCN training
ckpt_path  = '/content/drive/My Drive/fcn_model_best.h5'
final_path = '/content/drive/My Drive/fcn_model_final.h5'
callbacks = [
    ModelCheckpoint(ckpt_path, monitor='val_iou_metric', mode='max',
                    save_best_only=True, save_weights_only=False, verbose=1),
    ReduceLROnPlateau(monitor='val_iou_metric', mode='max',
                      factor=0.5, patience=5, min_lr=1e-6, verbose=1),
    EarlyStopping(monitor='val_iou_metric', mode='max',
                  patience=15, restore_best_weights=True, verbose=1),
]

# Warm-up training with frozen encoder
history1 = fcn.fit(
    x_train, y_train,
    validation_data=(x_val, y_val),
    batch_size=8,
    epochs=20,
    callbacks=callbacks,
    shuffle=True,
    verbose=1
)

# Fine-tuning: unfreeze deeper encoder layers and retrain
for layer in fcn.layers:
    if layer.name.startswith('block5'):
        layer.trainable = True

fcn.compile(optimizer=tf.keras.optimizers.Adam(1e-4),
            loss='binary_crossentropy',
            metrics=['accuracy', iou_metric, dice_coef])

history2 = fcn.fit(
    x_train, y_train,
    validation_data=(x_val, y_val),
    batch_size=8,
    epochs=80,
    callbacks=callbacks,
    shuffle=True,
    verbose=1
)

# Save the final FCN model to disk
fcn.save(final_path)
print(f"Best checkpoint: {ckpt_path}\nFinal model: {final_path}")
