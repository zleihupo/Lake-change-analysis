# -*- coding: utf-8 -*-
"""evaluate lake.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16kR4Qzt8aML-jnLPTHCkLKWMuiFE6Njr
"""

from google.colab import drive
# Mount Google Drive for file I/O
drive.mount('/content/drive')

import os, glob
import numpy as np
import tensorflow as tf
import cv2
from tqdm import tqdm
import pandas as pd
from sklearn.metrics import f1_score, jaccard_score, accuracy_score, precision_score, recall_score

# Paths to trained models
UNET_PATH   = '/content/drive/My Drive/unet_model_final.h5'
SEGNET_PATH = '/content/drive/My Drive/segnet_model_final.h5'
FCN_PATH    = '/content/drive/My Drive/fcn_model_final.h5'

# Directories for test and (optional) validation data
TEST_IMG_DIR = '/content/drive/My Drive/dataset/test/images/'
TEST_MASK_DIR= '/content/drive/My Drive/dataset/test/masks/'
VAL_IMG_DIR  = '/content/drive/My Drive/dataset/val/images/'   # optional
VAL_MASK_DIR = '/content/drive/My Drive/dataset/val/masks/'    # optional

IMG_SIZE = (256,256)
BATCH    = 64  # batch size used for inference

# Load images and corresponding binary masks; resize to IMG_SIZE, normalise to [0,1]
def load_data(img_dir, mask_dir, img_size=(256,256)):
    files = sorted(glob.glob(os.path.join(img_dir, '*')))
    X, y = [], []
    for p in tqdm(files, desc=f'Loading {os.path.basename(os.path.normpath(img_dir))}'):
        name = os.path.basename(p)
        mpath = os.path.join(mask_dir, name.replace('.jpg','.png'))
        im = cv2.imread(p); ms = cv2.imread(mpath, cv2.IMREAD_GRAYSCALE)
        if im is None or ms is None: continue
        im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)
        im = cv2.resize(im, img_size).astype(np.float32)/255.0
        ms = cv2.resize(ms, img_size, interpolation=cv2.INTER_NEAREST)
        ms = (ms>127).astype(np.uint8)
        X.append(im); y.append(ms)
    X = np.asarray(X, np.float32)
    y = np.asarray(y, np.uint8)
    return X, y

# Ensure predictions have shape (N, H, W) when models output (N, H, W, 1)
def squeeze(p): return p[...,0] if (p.ndim==4 and p.shape[-1]==1) else p

# Load cached predictions if available; otherwise run inference and cache

def get_or_predict(model_path, X, cache_name):
    npy = cache_name + '.npy'
    if os.path.exists(npy):
        return np.load(npy)
    model = tf.keras.models.load_model(model_path, compile=False)
    P = squeeze(model.predict(X, batch_size=BATCH, verbose=0))
    np.save(npy, P)
    return P

# Load test set once
x_test, y_test = load_data(TEST_IMG_DIR, TEST_MASK_DIR, IMG_SIZE)
y_test_flat = y_test.reshape(-1)  # flatten to (N*H*W,)

# Optionally load validation set for threshold tuning
x_val = y_val = None
if os.path.isdir(VAL_IMG_DIR) and os.path.isdir(VAL_MASK_DIR):
    x_val, y_val = load_data(VAL_IMG_DIR, VAL_MASK_DIR, IMG_SIZE)
    y_val_flat = y_val.reshape(-1)

# Obtain per-pixel probabilities for each model (from cache or inference)
p_unet = get_or_predict(UNET_PATH,  x_test, 'pred_unet_test')
p_seg  = get_or_predict(SEGNET_PATH,x_test, 'pred_segnet_test')
p_fcn  = get_or_predict(FCN_PATH,   x_test, 'pred_fcn_test')

if x_val is not None:
    p_unet_v = get_or_predict(UNET_PATH,  x_val, 'pred_unet_val')
    p_seg_v  = get_or_predict(SEGNET_PATH,x_val, 'pred_segnet_val')
    p_fcn_v  = get_or_predict(FCN_PATH,   x_val, 'pred_fcn_val')

# Vectorised metrics: returns (F1, IoU, Accuracy, Precision, Recall)
def metrics_from_flat(y_true_flat, y_pred_flat):
    # y_* are uint8 arrays with values {0,1}
    tp = np.logical_and(y_true_flat==1, y_pred_flat==1).sum()
    tn = np.logical_and(y_true_flat==0, y_pred_flat==0).sum()
    fp = np.logical_and(y_true_flat==0, y_pred_flat==1).sum()
    fn = np.logical_and(y_true_flat==1, y_pred_flat==0).sum()

    prec = tp / (tp+fp+1e-9)
    rec  = tp / (tp+fn+1e-9)
    acc  = (tp+tn) / (tp+tn+fp+fn+1e-9)
    iou  = tp / (tp+fp+fn+1e-9)
    f1   = 2*prec*rec / (prec+rec+1e-9)
    return f1, iou, acc, prec, rec

# Brute-force the decision threshold on validation probabilities (if provided)
def best_threshold_from_probs(p_flat, y_true_flat, lo=0.3, hi=0.7, steps=41):
    best_t, best_m = 0.5, -1
    for t in np.linspace(lo, hi, steps):
        yb = (p_flat > t).astype(np.uint8)
        f1, *_ = metrics_from_flat(y_true_flat, yb)
        if f1 > best_m:
            best_m, best_t = f1, float(t)
    return best_t, best_m

# Use validation-tuned threshold if available; otherwise fallback to 0.5
def tune_or_fixed(p_test, name):
    if x_val is None:
        return 0.5
    t,_ = best_threshold_from_probs(
        p_flat = p_unet_v.reshape(-1) if name=='U' else p_seg_v.reshape(-1) if name=='S' else p_fcn_v.reshape(-1),
        y_true_flat = y_val_flat
    )
    print(f"[{name}] best thr on val: {t:.3f}")
    return t

# Model-specific thresholds
t_unet = tune_or_fixed(p_unet, 'U')
t_seg  = tune_or_fixed(p_seg,  'S')
t_fcn  = tune_or_fixed(p_fcn,  'F')

# Evaluate single models at tuned thresholds
y_unet = (p_unet.reshape(-1) > t_unet).astype(np.uint8)
y_seg  = (p_seg.reshape(-1)  > t_seg ).astype(np.uint8)
y_fcn  = (p_fcn.reshape(-1)  > t_fcn ).astype(np.uint8)

print("\n[Single models @ tuned threshold]")
for name, yb in [('U-Net',y_unet),('SegNet',y_seg),('FCN',y_fcn)]:
    f1,iou,acc,prec,rec = metrics_from_flat(y_test_flat, yb)
    print(f"{name:6s} → F1={f1:.4f}, IoU={iou:.4f}, Acc={acc:.4f}, Prec={prec:.4f}, Rec={rec:.4f}")

# Soft-voting ensemble: grid-search over weights (wu, ws, wf) with wu+ws+wf=1
weights = np.arange(0.0, 1.0001, 0.1)
rows, best = [], {'F1':-1}

# Determine ensemble threshold on validation set (if available)

def ens_thr(wu,ws,wf):
    if x_val is None: return 0.5
    p_ens_v = wu*p_unet_v + ws*p_seg_v + wf*p_fcn_v
    t,_ = best_threshold_from_probs(p_ens_v.reshape(-1), y_val_flat)
    return t

# Search across all valid weight combinations
for wu in weights:
    for ws in weights:
        if wu+ws>1.0: continue
        wf = 1.0 - wu - ws
        t = ens_thr(wu,ws,wf)

        p_ens = wu*p_unet + ws*p_seg + wf*p_fcn
        yb = (p_ens.reshape(-1) > t).astype(np.uint8)

        f1,iou,acc,prec,rec = metrics_from_flat(y_test_flat, yb)
        rows.append([wu,ws,wf,t,f1,iou,acc,prec,rec])
        if f1>best['F1']:
            best = {"wu":wu,"ws":ws,"wf":wf,"t":t,"F1":f1,"IoU":iou,"Acc":acc,"Prec":prec,"Rec":rec}

# Save ensemble search results
columns = ["U-Net","SegNet","FCN","Thresh","F1","IoU","Accuracy","Precision","Recall"]
df = pd.DataFrame(rows, columns=columns)
df = df.sort_values('F1', ascending=False).reset_index(drop=True)
df.to_csv("grid_search_ensemble_results.csv", index=False)

print("\nBest (F1): (wu,ws,wf) =", (best['wu'],best['ws'],best['wf']),
      " thr=", round(best['t'],3))
print("   → F1={F1:.4f}, IoU={IoU:.4f}, Acc={Acc:.4f}, Prec={Prec:.4f}, Rec={Rec:.4f}".format(**best))
print("Saved: grid_search_ensemble_results.csv")

# Persist predictions for reuse (optional)
np.save("pred_unet_test.npy", p_unet)
np.save("pred_segnet_test.npy", p_seg)
np.save("pred_fcn_test.npy",  p_fcn)
if x_val is not None:
    np.save("pred_unet_val.npy", p_unet_v)
    np.save("pred_segnet_val.npy", p_seg_v)
    np.save("pred_fcn_val.npy",  p_fcn_v)

# Visualise results: ensemble heatmap and model comparison bar chart
import matplotlib.pyplot as plt

# Heatmap of F1 over (wu, ws) with wf=1-wu-ws
w_vals = np.round(np.arange(0.0, 1.0001, 0.1), 1)
grid = np.full((len(w_vals), len(w_vals)), np.nan, dtype=float)

for _, r in df.iterrows():
    wu, ws, f1 = round(r["U-Net"],1), round(r["SegNet"],1), r["F1"]
    xi = int(round(wu*10)); yi = int(round(ws*10))
    grid[yi, xi] = max(grid[yi, xi], f1) if not np.isnan(grid[yi, xi]) else f1

plt.figure(figsize=(8,6))
im = plt.imshow(grid, origin='lower', extent=[0,1,0,1], aspect='equal')
plt.colorbar(im, label='F1 Score')
plt.title('Soft Voting Ensemble — F1 Heatmap')
plt.xlabel('U-Net weight'); plt.ylabel('SegNet weight')
# Optional contour lines for readability
cs = plt.contour(np.linspace(0,1,grid.shape[1]), np.linspace(0,1,grid.shape[0]),
                 np.nan_to_num(grid, nan=-1), levels=8, linewidths=0.5)
plt.clabel(cs, inline=True, fontsize=8)
plt.tight_layout()
plt.savefig('ensemble_heatmap.png', dpi=300)
plt.close()

# Bar chart: single models vs best ensemble
u_f1,u_iou,u_acc,u_prec,u_rec = metrics_from_flat(y_test_flat, y_unet)
s_f1,s_iou,s_acc,s_prec,s_rec = metrics_from_flat(y_test_flat, y_seg)
f_f1,f_iou,f_acc,f_prec,f_rec = metrics_from_flat(y_test_flat, y_fcn)

ens_scores = [best["F1"], best["IoU"], best["Acc"], best["Prec"], best["Rec"]]
unet_scores = [u_f1,u_iou,u_acc,u_prec,u_rec]
seg_scores  = [s_f1,s_iou,s_acc,s_prec,s_rec]
fcn_scores  = [f_f1,f_iou,f_acc,f_prec,f_rec]

labels = ["F1","IoU","Accuracy","Precision","Recall"]
x = np.arange(len(labels)); w = 0.2

plt.figure(figsize=(10,6))
plt.bar(x - 1.5*w, unet_scores, width=w, label='U-Net')
plt.bar(x - 0.5*w, seg_scores,  width=w, label='SegNet')
plt.bar(x + 0.5*w, fcn_scores,  width=w, label='FCN')
plt.bar(x + 1.5*w, ens_scores,  width=w, label='Ensemble (best)')
plt.xticks(x, labels); plt.ylim(0,1.0)
plt.title('Single Models vs Best Ensemble')
plt.ylabel('Score'); plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.legend(loc='lower right')
plt.tight_layout()
plt.savefig('model_vs_ensemble.png', dpi=300)
plt.close()

print("Saved figures: ensemble_heatmap.png, model_vs_ensemble.png")

# Package outputs and trigger download (Colab)
import zipfile
from google.colab import files

files_to_zip = [
    "grid_search_ensemble_results.csv",
    "ensemble_heatmap.png",
    "model_vs_ensemble.png",
    "pred_unet_test.npy",
    "pred_segnet_test.npy",
    "pred_fcn_test.npy"
]

files_existing = [f for f in files_to_zip if os.path.exists(f)]
zip_path = "/content/evaluation_results.zip"
with zipfile.ZipFile(zip_path, 'w') as zf:
    for f in files_existing:
        zf.write(f, arcname=os.path.basename(f))

files.download(zip_path)

!ls -lh /content/evaluation_results.zip
