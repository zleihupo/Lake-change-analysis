# -*- coding: utf-8 -*-
"""plot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kYVE6RijS_qQJRaco_o-5M7tFvEqldRK
"""

from google.colab import drive
drive.mount('/content/drive')

# Choose "drive" or "upload"
MODE = "updolad"  # "drive" or "upload"

# When MODE="drive", set your Drive root directory (modify to your path)
DRIVE_ROOT = "/content/drive/MyDrive"  # ← modify to yours

# Whether to also copy generated figures back to DRIVE_ROOT
SAVE_TO_DRIVE = True

!sudo apt-get update -y
!sudo apt-get install -y chromium-chromedriver
!pip install -q selenium pandas matplotlib opencv-python

import os, glob, zipfile, time, json, random
import numpy as np
import pandas as pd
import cv2
import matplotlib.pyplot as plt

if MODE == "drive":
    from google.colab import drive
    drive.mount('/content/drive')
    # Convention file paths (adjust according to your directory structure)
    TEST_IMG_DIR  = f"{DRIVE_ROOT}/dataset/test/images/"        # or images/
    TEST_MASK_DIR = f"{DRIVE_ROOT}/dataset/test/masks/"         # or masks/
    UNET_NPY      = f"{DRIVE_ROOT}/evaluation_results/pred_unet_test.npy"
    SEGNET_NPY    = f"{DRIVE_ROOT}/evaluation_results/pred_segnet_test.npy"
    FCN_NPY       = f"{DRIVE_ROOT}/evaluation_results/pred_fcn_test.npy"
    # Map HTML (fuzzy match, no need to change)
    GLOBAL_HTML = next((f for f in glob.glob(f"{DRIVE_ROOT}/lake_analysis_outputs/global_heatmap_lakes_trends (1).html") if "global" in f.lower() or "heatmap" in f.lower()), None)
    REGION_HTML = next((f for f in glob.glob(f"{DRIVE_ROOT}/lake_analysis_outputs/region_trends_map (1).html") if "region" in f.lower()), None)
    # Time series and importance CSV (adjust if needed)
    TS_CSV = next((f for f in glob.glob(f"{DRIVE_ROOT}/lake_analysis_outputs/100Lake_area_Temperature_2000-2025.csv") if "Lake" in f or "Temperature" in f or "2000" in f), None)
    PERM_CSV = next((f for f in glob.glob(f"{DRIVE_ROOT}/lake_analysis_outputs/Permutation_importance（by_region_summer）.csv") if "Permutation" in f), None)
    TOP3_CSV = next((f for f in glob.glob(f"{DRIVE_ROOT}/lake_analysis_outputs/Top-3_features_by_region_summer.csv") if "Top-3" in f or "Top3" in f), None)

else:
    from google.colab import files
    print("Please select the required files (multiple selection allowed): test images/masks ZIP, NPY, HTML, CSV etc.")
    uploaded = files.upload()
    # If zip is uploaded, unzip it
    def unzip_if_exists(zipname, outdir):
        if os.path.exists(zipname):
            os.makedirs(outdir, exist_ok=True)
            with zipfile.ZipFile(zipname, 'r') as zf:
                zf.extractall(outdir)
            print(f"Unzipped: {zipname} -> {outdir}")
    unzip_if_exists("test_images.zip", "test_images")
    unzip_if_exists("test_masks.zip", "test_masks")

    # Guess directories
    def guess_dir(root):
        if os.path.isdir(root): return root
        for d in glob.glob("*"):
            if os.path.isdir(d) and ("image" in d.lower() or "mask" in d.lower()):
                return d
        return root
    TEST_IMG_DIR  = guess_dir("test_images")
    TEST_MASK_DIR = guess_dir("test_masks")

    # Guess NPY/HTML/CSV files
    UNET_NPY   = next((k for k in uploaded if "unet"   in k and k.endswith(".npy")), None)
    SEGNET_NPY = next((k for k in uploaded if "segnet" in k and k.endswith(".npy")), None)
    FCN_NPY    = next((k for k in uploaded if "fcn"    in k and k.endswith(".npy")), None)
    BEST_JSON  = "best_ensemble.json" if "best_ensemble.json" in uploaded else None
    GRID_CSV   = next((k for k in uploaded if k.endswith(".csv") and "grid" in k.lower()), None)

    GLOBAL_HTML = next((k for k in uploaded if k.endswith(".html") and ("global" in k.lower() or "heatmap" in k.lower())), None)
    REGION_HTML = next((k for k in uploaded if k.endswith(".html") and "region" in k.lower()), None)
    TS_CSV  = next((k for k in uploaded if k.endswith(".csv") and ("Lake" in k or "Temperature" in k or "2000" in k)), None)
    PERM_CSV = next((k for k in uploaded if k.endswith(".csv") and "Permutation" in k), None)
    TOP3_CSV = next((k for k in uploaded if k.endswith(".csv") and ("Top_3" in k or "Top3" in k)), None)

print("IMG_DIR:", TEST_IMG_DIR)
print("MSK_DIR:", TEST_MASK_DIR)
print("NPY:", UNET_NPY, SEGNET_NPY, FCN_NPY)
print("HTML:", GLOBAL_HTML, REGION_HTML)
print("TS_CSV:", TS_CSV, "PERM_CSV:", PERM_CSV, "TOP3_CSV:", TOP3_CSV)

def read_img_rgb(path, size=None):
    img = cv2.imread(path);
    if img is None: raise FileNotFoundError(path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    if size and img.shape[:2] != size:
        img = cv2.resize(img, size, interpolation=cv2.INTER_AREA)
    return (img.astype(np.float32)/255.)

def read_mask_bin(path, size=None):
    m = cv2.imread(path, cv2.IMREAD_GRAYSCALE)
    if m is None: raise FileNotFoundError(path)
    if size and m.shape[:2] != size:
        m = cv2.resize(m, size, interpolation=cv2.INTER_NEAREST)
    return (m>127).astype(np.uint8)

def load_pairs(img_dir, msk_dir):
    img_paths = sorted(glob.glob(os.path.join(img_dir, "*")))
    pairs = []
    for ip in img_paths:
        stem = os.path.splitext(os.path.basename(ip))[0]
        mp = None
        for ext in (".png",".jpg",".jpeg",".tif",".tiff",".bmp"):
            p = os.path.join(msk_dir, stem+ext)
            if os.path.exists(p): mp = p; break
        if mp: pairs.append((ip, mp))
    if not pairs: raise RuntimeError("No (image, mask) pairs found, please check image and mask directories")
    return pairs

# HTML → PNG screenshot
def html_to_png(html_path, out_png="map.png", width=1600, height=1000, wait_sec=3):
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    chrome_options = Options()
    chrome_options.add_argument("--headless=new")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument(f"--window-size={width},{height}")
    driver = webdriver.Chrome(options=chrome_options)
    driver.get("file://" + os.path.abspath(html_path))
    time.sleep(wait_sec)
    driver.save_screenshot(out_png)
    driver.quit()
    print("Saved:", out_png)

pairs = load_pairs(TEST_IMG_DIR, TEST_MASK_DIR)

def save_triptych(img_path, msk_path, out_path, size=(256, 256), show=True):
    # Read image and optionally resize; normalize RGB to [0,1]
    img = read_img_rgb(img_path, size=size)
    # Read mask and optionally resize; convert to binary (0/1), use nearest neighbor to keep edges sharp
    msk = read_mask_bin(msk_path, size=size)

    # One row three columns: Image / GSW mask / overlay
    fig, ax = plt.subplots(1, 3, figsize=(9, 3))
    ax[0].imshow(img);                 ax[0].set_title("Image");     ax[0].axis("off")
    ax[1].imshow(msk, cmap="gray");    ax[1].set_title("GSW mask");  ax[1].axis("off")
    ax[2].imshow(img);
    ax[2].imshow(msk, cmap="gray", alpha=0.35)  # Semi-transparent overlay for alignment check
    ax[2].set_title("Aligned pair");   ax[2].axis("off")

    plt.tight_layout()
    plt.savefig(out_path, dpi=300)     # Save before show to avoid margin issues
    if show:
        plt.show()                     # Show must be before close
    plt.close(fig)                     # Release memory (important for batch figure generation)
    print("Saved:", out_path)

# Example: generate Fig.2 using the 969th sample
save_triptych(pairs[968][0], pairs[968][1], "Fig2_sample_triptych.png", size=(256, 256), show=True)

# Load probability cache
pred_unet   = np.load(UNET_NPY).squeeze()
pred_segnet = np.load(SEGNET_NPY).squeeze()
pred_fcn    = np.load(FCN_NPY).squeeze()
N = len(pairs)
assert pred_unet.shape[0]==pred_segnet.shape[0]==pred_fcn.shape[0]==N, "NPY count not consistent with samples"

# Best (w, τ)
best = {"w":[0.3,0.3,0.4], "tau_ens":0.39, "tau_unet":0.5, "tau_segnet":0.5, "tau_fcn":0.5}

wU, wS, wF = best["w"]
tauU = best.get("tau_unet",0.5); tauS = best.get("tau_segnet",0.5); tauF = best.get("tau_fcn",0.5); tauE = best.get("tau_ens",0.5)

# Select first/middle/last three samples
idxs = [4, 298, 1135] if N>=3 else [0]
H,W = pred_unet.shape[1], pred_unet.shape[2]

fig, axes = plt.subplots(len(idxs), 6, figsize=(24, 3*len(idxs)))
if len(idxs)==1: axes = np.array([axes])

for r, idx in enumerate(idxs):
    ip, mp = pairs[idx]
    img = read_img_rgb(ip, (H,W))
    gt  = read_mask_bin(mp, (H,W))
    pU, pS, pF = pred_unet[idx], pred_segnet[idx], pred_fcn[idx]
    pE = wU*pU + wS*pS + wF*pF
    mU = (pU>=tauU).astype(np.uint8)
    mS = (pS>=tauS).astype(np.uint8)
    mF = (pF>=tauF).astype(np.uint8)
    mE = (pE>=tauE).astype(np.uint8)

    axes[r,0].imshow(img);            axes[r,0].set_title("Image");    axes[r,0].axis("off")
    axes[r,1].imshow(gt, cmap="gray");axes[r,1].set_title("GSW mask"); axes[r,1].axis("off")
    axes[r,2].imshow(mU, cmap="gray");axes[r,2].set_title("U-Net");    axes[r,2].axis("off")
    axes[r,3].imshow(mS, cmap="gray");axes[r,3].set_title("SegNet");   axes[r,3].axis("off")
    axes[r,4].imshow(mF, cmap="gray");axes[r,4].set_title("FCN");      axes[r,4].axis("off")
    axes[r,5].imshow(mE, cmap="gray");axes[r,5].set_title(f"Ensemble\nw=({wU},{wS},{wF}), τ={tauE}"); axes[r,5].axis("off")

plt.tight_layout()
plt.savefig("Fig3_qualitative_comparison.png", dpi=300)
plt.show()
plt.close(fig)
print("Saved: Fig3_qualitative_comparison.png")

if GLOBAL_HTML and os.path.exists(GLOBAL_HTML):
    html_to_png(GLOBAL_HTML, "Fig6_global_trends.png", width=1600, height=1000, wait_sec=3)
    img = cv2.cvtColor(cv2.imread("Fig6_global_trends.png"), cv2.COLOR_BGR2RGB)
    plt.figure(figsize=(12,7)); plt.imshow(img); plt.axis('off')
    plt.title("Fig.6 Global trends (screenshot)");
    plt.show()  # <—— show

if REGION_HTML and os.path.exists(REGION_HTML):
    html_to_png(REGION_HTML, "Fig7_region_trends.png", width=1600, height=1000, wait_sec=3)
    img = cv2.cvtColor(cv2.imread("Fig7_region_trends.png"), cv2.COLOR_BGR2RGB)
    plt.figure(figsize=(12,7)); plt.imshow(img); plt.axis('off')
    plt.title("Fig.7 Regional trends (screenshot)")
    plt.show()  # <—— show

# ===== Fig.8 (robust version): automatically compute area_index and plot region small multiples =====
import os, numpy as np, pandas as pd, matplotlib.pyplot as plt, math

# If auto-search failed above, manually specify CSV path:
# TS_CSV = "/content/drive/MyDrive/your_project_folder/100Lake_area_Temperature_2000-2025.csv"

assert TS_CSV is not None and os.path.exists(TS_CSV), f"Time-series CSV not found: {TS_CSV}"
df = pd.read_csv(TS_CSV)

# 1) Infer year column
if "year" not in df.columns:
    if "date" in df.columns:
        df["year"] = pd.to_datetime(df["date"]).dt.year
    elif "Year" in df.columns:
        df["year"] = df["Year"]
    else:
        raise ValueError("CSV must contain year or date column (cannot infer year).")

# 2) Infer lake / region column names
def find_col(cands):
    for c in cands:
        if c in df.columns: return c
    for col in df.columns:
        l = col.lower()
        if any(k in l for k in cands): return col
    return None

lake_col   = find_col(["lake","lakename","name","lake","location"])
region_col = find_col(["region","zone","area","region","district"])
if lake_col is None:
    # If no lake column, fallback to global baseline
    print("No lake column detected, will use global baseline of all samples.")
if region_col is None:
    print("No region column detected, all assigned to 'All'.")
    region_col = "region"
    df[region_col] = "All"

# 3) Infer area column (choose one)
area_candidates = ["area_index","area_m2","area_km2","area","Area","AREA"]
area_col = None
for c in area_candidates:
    if c in df.columns:
        area_col = c
        break
if area_col is None:
    # Loose match
    for col in df.columns:
        l = col.lower()
        if "area" in l:
            area_col = col; break
if area_col is None:
    raise ValueError("CSV missing area column (e.g., area_m2/area_km2/area).")

print(f"Detected columns: year={'year'}, region={region_col}, lake={lake_col}, area={area_col}")

# 4) If area_index not present, compute area_index
if "area_index" not in df.columns:
    # Convert area to numeric
    df[area_col] = pd.to_numeric(df[area_col], errors="coerce")
    df = df.dropna(subset=[area_col, "year"])

    BASELINE_YEARS = {2000,2001,2002}

    def compute_baseline(g):
        # Prefer mean of 2000–2002; if missing, fallback to first 3 records of that lake
        cand = g[g["year"].isin(BASELINE_YEARS)][area_col]
        if cand.dropna().shape[0] >= 1:
            return float(cand.mean())
        return float(g.sort_values("year")[area_col].head(3).mean())

    if lake_col is not None:
        base_map = df.groupby(lake_col).apply(compute_baseline).to_dict()
        df["__baseline__"] = df[lake_col].map(base_map)
    else:
        # Without lake column: use global baseline (2000–2002 or first 3 records)
        all_base = compute_baseline(df.copy())
        df["__baseline__"] = all_base

    df = df[df["__baseline__"] > 0]
    df["area_index"] = df[area_col] / df["__baseline__"]
else:
    # If area_index already exists, ensure numeric
    df["area_index"] = pd.to_numeric(df["area_index"], errors="coerce")
    df = df.dropna(subset=["area_index"])

# 5) Aggregate to region-year and plot
g = df.groupby([region_col,"year"], as_index=False)["area_index"].mean()

regions = sorted(g[region_col].unique().tolist())
ncol = min(3, len(regions)) if len(regions)>0 else 1
nrow = math.ceil(len(regions)/ncol) if len(regions)>0 else 1

fig, axes = plt.subplots(nrow, ncol, figsize=(5*ncol, 3.2*nrow), squeeze=False)

for i, reg in enumerate(regions):
    r, c = divmod(i, ncol)
    gi = g[g[region_col]==reg].sort_values("year")
    axes[r,c].plot(gi["year"], gi["area_index"], marker='o')
    if gi.shape[0] >= 2:
        x = gi["year"].values.astype(float); y = gi["area_index"].values.astype(float)
        k = np.polyfit(x, y, 1)
        axes[r,c].plot(x, k[0]*x + k[1], linestyle="--")
    axes[r,c].set_title(reg)
    axes[r,c].set_xlabel("Year"); axes[r,c].set_ylabel("Summer area index")
    axes[r,c].grid(True, linestyle="--", alpha=0.3)

# Close extra subplots
for j in range(len(regions), nrow*ncol):
    r, c = divmod(j, ncol); axes[r,c].axis("off")

plt.tight_layout()
plt.savefig("Fig8_region_small_multiples.png", dpi=300)
plt.show()
plt.close(fig)
print("Saved: Fig8_region_small_multiples.png")

out9 = None
if PERM_CSV and os.path.exists(PERM_CSV):
    dfp = pd.read_csv(PERM_CSV)
    region_col = next((c for c in dfp.columns if "region" in c.lower()), None)
    feat_col   = next((c for c in dfp.columns if "feature" in c.lower()), None)
    imp_col    = next((c for c in dfp.columns if "importance" in c.lower()), None)
    if region_col and feat_col and imp_col:
        gg = (dfp[[region_col, feat_col, imp_col]]
              .groupby(region_col, as_index=False)
              .apply(lambda x: x.sort_values(imp_col, ascending=False).head(5))
              .reset_index(drop=True))
        regs = gg[region_col].unique().tolist()
        ncol = min(3, len(regs)); nrow = (len(regs)+ncol-1)//ncol
        fig, axes = plt.subplots(nrow, ncol, figsize=(5*ncol, 3.5*nrow), squeeze=False)
        for i, reg in enumerate(regs):
            r,c = divmod(i, ncol)
            gi = gg[gg[region_col]==reg]
            axes[r,c].barh(gi[feat_col], gi[imp_col])
            axes[r,c].invert_yaxis()
            axes[r,c].set_title(reg); axes[r,c].set_xlabel("Permutation importance")
        for j in range(len(regs), nrow*ncol): r,c = divmod(j, ncol); axes[r,c].axis("off")
        plt.tight_layout(); out9 = "Fig9_permutation_importance.png"; plt.savefig(out9, dpi=300); plt.show(); plt.close(fig)
        print("Saved:", out9)

elif TOP3_CSV and os.path.exists(TOP3_CSV):
    dft = pd.read_csv(TOP3_CSV)
    region_col = next((c for c in dft.columns if "region" in c.lower()), None)
    feat_col   = next((c for c in dft.columns if "feature" in c.lower()), None)
    score_col  = next((c for c in dft.columns if "score" in c.lower()), None)
    if region_col and feat_col and score_col:
        regs = dft[region_col].unique().tolist()
        ncol = min(3, len(regs)); nrow = (len(regs)+ncol-1)//ncol
        fig, axes = plt.subplots(nrow, ncol, figsize=(5*ncol, 3.5*nrow), squeeze=False)
        for i, reg in enumerate(regs):
            r,c = divmod(i, ncol)
            gi = dft[dft[region_col]==reg]
            axes[r,c].barh(gi[feat_col], gi[score_col])
            axes[r,c].invert_yaxis()
            axes[r,c].set_title(reg); axes[r,c].set_xlabel("Top-3 score")
        for j in range(len(regs), nrow*ncol): r,c = divmod(j, ncol); axes[r,c].axis("off")
        plt.tight_layout(); out9 = "Fig9_top3_features.png"; plt.savefig(out9, dpi=300); plt.show(); plt.close(fig)
        print("Saved:", out9)
else:
    print("Importance/Top-3 CSV not found, skipping Fig.9")

from google.colab import files
outs = ["Fig2_sample_triptych.png","Fig3_qualitative_comparison.png",
        "Fig6_global_trends.png","Fig7_region_trends.png",
        "Fig8_region_small_multiples.png",
        "Fig9_permutation_importance.png","Fig9_top3_features.png"]
outs = [f for f in outs if os.path.exists(f)]

if SAVE_TO_DRIVE and MODE=="drive":
    import shutil, os
    out_dir = os.path.join(DRIVE_ROOT, "figs_out")
    os.makedirs(out_dir, exist_ok=True)
    for f in outs:
        shutil.copy2(f, os.path.join(out_dir, f))
    print("Copied to:", out_dir)

print("Select the images you want to download (multiple allowed):")
for f in outs:
    files.download(f)

