# -*- coding: utf-8 -*-
"""plot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kYVE6RijS_qQJRaco_o-5M7tFvEqldRK
"""

from google.colab import drive
drive.mount('/content/drive')

# Select data source mode: "drive" (read from Google Drive) or "upload" (manual file upload).
# NOTE: current value "updolad" appears to be a typo; expected "upload" or "drive".
MODE = "updolad"  # "drive" or "upload"

# Root directory used when MODE == "drive". Adjust to your own Drive path.
DRIVE_ROOT = "/content/drive/MyDrive"  # update if your folder differs

# If True, copy generated figures to DRIVE_ROOT/figs_out when MODE == "drive".
SAVE_TO_DRIVE = True

# System and Python dependencies for HTML screenshotting and plotting
!sudo apt-get update -y
!sudo apt-get install -y chromium-chromedriver
!pip install -q selenium pandas matplotlib opencv-python

import os, glob, zipfile, time, json, random
import numpy as np
import pandas as pd
import cv2
import matplotlib.pyplot as plt

if MODE == "drive":
    from google.colab import drive
    drive.mount('/content/drive')
    # Canonical locations for test data and cached predictions
    TEST_IMG_DIR  = f"{DRIVE_ROOT}/dataset/test/images/"
    TEST_MASK_DIR = f"{DRIVE_ROOT}/dataset/test/masks/"
    UNET_NPY      = f"{DRIVE_ROOT}/evaluation_results/pred_unet_test.npy"
    SEGNET_NPY    = f"{DRIVE_ROOT}/evaluation_results/pred_segnet_test.npy"
    FCN_NPY       = f"{DRIVE_ROOT}/evaluation_results/pred_fcn_test.npy"
    # Optional interactive HTML outputs (best-effort fuzzy matching)
    GLOBAL_HTML = next((f for f in glob.glob(f"{DRIVE_ROOT}/lake_analysis_outputs/global_heatmap_lakes_trends (1).html") if "global" in f.lower() or "heatmap" in f.lower()), None)
    REGION_HTML = next((f for f in glob.glob(f"{DRIVE_ROOT}/lake_analysis_outputs/region_trends_map (1).html") if "region" in f.lower()), None)
    # Optional time-series / feature-importance CSVs (edit patterns if needed)
    TS_CSV = next((f for f in glob.glob(f"{DRIVE_ROOT}/lake_analysis_outputs/100Lake_area_Temperature_2000-2025.csv") if "Lake" in f or "Temperature" in f or "2000" in f), None)
    PERM_CSV = next((f for f in glob.glob(f"{DRIVE_ROOT}/lake_analysis_outputs/Permutation_importance（by_region_summer）.csv") if "Permutation" in f), None)
    TOP3_CSV = next((f for f in glob.glob(f"{DRIVE_ROOT}/lake_analysis_outputs/Top-3_features_by_region_summer.csv") if "Top-3" in f or "Top3" in f), None)

else:
    from google.colab import files
    print("Please select required files (NPY/HTML/CSV and optionally zipped test images/masks).")
    uploaded = files.upload()

    # Unzip helper for user-provided archives
    def unzip_if_exists(zipname, outdir):
        if os.path.exists(zipname):
            os.makedirs(outdir, exist_ok=True)
            with zipfile.ZipFile(zipname, 'r') as zf:
                zf.extractall(outdir)
            print(f"Unzipped: {zipname} -> {outdir}")
    unzip_if_exists("test_images.zip", "test_images")
    unzip_if_exists("test_masks.zip", "test_masks")

    # Heuristics to locate image/mask folders after upload
    def guess_dir(root):
        if os.path.isdir(root): return root
        for d in glob.glob("*"):
            if os.path.isdir(d) and ("image" in d.lower() or "mask" in d.lower()):
                return d
        return root
    TEST_IMG_DIR  = guess_dir("test_images")
    TEST_MASK_DIR = guess_dir("test_masks")

    # Try to identify uploaded NPY/HTML/CSV artefacts
    UNET_NPY   = next((k for k in uploaded if "unet"   in k and k.endswith(".npy")), None)
    SEGNET_NPY = next((k for k in uploaded if "segnet" in k and k.endswith(".npy")), None)
    FCN_NPY    = next((k for k in uploaded if "fcn"    in k and k.endswith(".npy")), None)
    BEST_JSON  = "best_ensemble.json" if "best_ensemble.json" in uploaded else None
    GRID_CSV   = next((k for k in uploaded if k.endswith(".csv") and "grid" in k.lower()), None)

    GLOBAL_HTML = next((k for k in uploaded if k.endswith(".html") and ("global" in k.lower() or "heatmap" in k.lower())), None)
    REGION_HTML = next((k for k in uploaded if k.endswith(".html") and "region" in k.lower()), None)
    TS_CSV  = next((k for k in uploaded if k.endswith(".csv") and ("Lake" in k or "Temperature" in k or "2000" in k)), None)
    PERM_CSV = next((k for k in uploaded if k.endswith(".csv") and "Permutation" in k), None)
    TOP3_CSV = next((k for k in uploaded if k.endswith(".csv") and ("Top_3" in k or "Top3" in k)), None)

print("IMG_DIR:", TEST_IMG_DIR)
print("MSK_DIR:", TEST_MASK_DIR)
print("NPY:", UNET_NPY, SEGNET_NPY, FCN_NPY)
print("HTML:", GLOBAL_HTML, REGION_HTML)
print("TS_CSV:", TS_CSV, "PERM_CSV:", PERM_CSV, "TOP3_CSV:", TOP3_CSV)

def read_img_rgb(path, size=None):
    # Read BGR image, convert to RGB, optional resize, normalise to [0,1]
    img = cv2.imread(path);
    if img is None: raise FileNotFoundError(path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    if size and img.shape[:2] != size:
        img = cv2.resize(img, size, interpolation=cv2.INTER_AREA)
    return (img.astype(np.float32)/255.)

def read_mask_bin(path, size=None):
    # Read grayscale mask, optional resize, then binarise (0/1) at threshold 127
    m = cv2.imread(path, cv2.IMREAD_GRAYSCALE)
    if m is None: raise FileNotFoundError(path)
    if size and m.shape[:2] != size:
        m = cv2.resize(m, size, interpolation=cv2.INTER_NEAREST)
    return (m>127).astype(np.uint8)

def load_pairs(img_dir, msk_dir):
    # Match images to masks by filename stem; support common extensions
    img_paths = sorted(glob.glob(os.path.join(img_dir, "*")))
    pairs = []
    for ip in img_paths:
        stem = os.path.splitext(os.path.basename(ip))[0]
        mp = None
        for ext in (".png",".jpg",".jpeg",".tif",".tiff",".bmp"):
            p = os.path.join(msk_dir, stem+ext)
            if os.path.exists(p): mp = p; break
        if mp: pairs.append((ip, mp))
    if not pairs: raise RuntimeError("No (image, mask) pairs found; check directories and filenames.")
    return pairs

# Render a static screenshot (PNG) from a local HTML file using headless Chrome
def html_to_png(html_path, out_png="map.png", width=1600, height=1000, wait_sec=3):
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    chrome_options = Options()
    chrome_options.add_argument("--headless=new")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument(f"--window-size={width},{height}")
    driver = webdriver.Chrome(options=chrome_options)
    driver.get("file://" + os.path.abspath(html_path))
    time.sleep(wait_sec)
    driver.save_screenshot(out_png)
    driver.quit()
    print("Saved:", out_png)

pairs = load_pairs(TEST_IMG_DIR, TEST_MASK_DIR)

def save_triptych(img_path, msk_path, out_path, size=(256, 256), show=True):
    # Create a 3-panel figure: image, mask, and overlay for qualitative alignment
    img = read_img_rgb(img_path, size=size)
    msk = read_mask_bin(msk_path, size=size)

    fig, ax = plt.subplots(1, 3, figsize=(9, 3))
    ax[0].imshow(img);                 ax[0].set_title("Image");     ax[0].axis("off")
    ax[1].imshow(msk, cmap="gray");    ax[1].set_title("GSW mask");  ax[1].axis("off")
    ax[2].imshow(img);
    ax[2].imshow(msk, cmap="gray", alpha=0.35)  # semi-transparent overlay
    ax[2].set_title("Aligned pair");   ax[2].axis("off")

    plt.tight_layout()
    plt.savefig(out_path, dpi=300)
    if show:
        plt.show()
    plt.close(fig)
    print("Saved:", out_path)

# Example panel (Fig.2) using the 969th sample
save_triptych(pairs[968][0], pairs[968][1], "Fig2_sample_triptych.png", size=(256, 256), show=True)

# Load cached per-pixel probabilities for each model; assert alignment with pairs
pred_unet   = np.load(UNET_NPY).squeeze()
pred_segnet = np.load(SEGNET_NPY).squeeze()
pred_fcn    = np.load(FCN_NPY).squeeze()
N = len(pairs)
assert pred_unet.shape[0]==pred_segnet.shape[0]==pred_fcn.shape[0]==N, "NPY counts must match number of samples"

# Pre-selected ensemble weights and thresholds (τ for individual and ensemble outputs)
best = {"w":[0.3,0.3,0.4], "tau_ens":0.39, "tau_unet":0.5, "tau_segnet":0.5, "tau_fcn":0.5}

wU, wS, wF = best["w"]
tauU = best.get("tau_unet",0.5); tauS = best.get("tau_segnet",0.5); tauF = best.get("tau_fcn",0.5); tauE = best.get("tau_ens",0.5)

# Choose representative samples for qualitative comparison
idxs = [4, 298, 1135] if N>=3 else [0]
H,W = pred_unet.shape[1], pred_unet.shape[2]

fig, axes = plt.subplots(len(idxs), 6, figsize=(24, 3*len(idxs)))
if len(idxs)==1: axes = np.array([axes])

for r, idx in enumerate(idxs):
    ip, mp = pairs[idx]
    img = read_img_rgb(ip, (H,W))
    gt  = read_mask_bin(mp, (H,W))
    pU, pS, pF = pred_unet[idx], pred_segnet[idx], pred_fcn[idx]
    pE = wU*pU + wS*pS + wF*pF
    mU = (pU>=tauU).astype(np.uint8)
    mS = (pS>=tauS).astype(np.uint8)
    mF = (pF>=tauF).astype(np.uint8)
    mE = (pE>=tauE).astype(np.uint8)

    axes[r,0].imshow(img);            axes[r,0].set_title("Image");    axes[r,0].axis("off")
    axes[r,1].imshow(gt, cmap="gray");axes[r,1].set_title("GSW mask"); axes[r,1].axis("off")
    axes[r,2].imshow(mU, cmap="gray");axes[r,2].set_title("U-Net");    axes[r,2].axis("off")
    axes[r,3].imshow(mS, cmap="gray");axes[r,3].set_title("SegNet");   axes[r,3].axis("off")
    axes[r,4].imshow(mF, cmap="gray");axes[r,4].set_title("FCN");      axes[r,4].axis("off")
    axes[r,5].imshow(mE, cmap="gray");axes[r,5].set_title(f"Ensemble\nw=({wU},{wS},{wF}), τ={tauE}"); axes[r,5].axis("off")

plt.tight_layout()
plt.savefig("Fig3_qualitative_comparison.png", dpi=300)
plt.show()
plt.close(fig)
print("Saved: Fig3_qualitative_comparison.png")

# Optional: convert interactive HTML maps to static PNG for inclusion in reports
if GLOBAL_HTML and os.path.exists(GLOBAL_HTML):
    html_to_png(GLOBAL_HTML, "Fig6_global_trends.png", width=1600, height=1000, wait_sec=3)
    img = cv2.cvtColor(cv2.imread("Fig6_global_trends.png"), cv2.COLOR_BGR2RGB)
    plt.figure(figsize=(12,7)); plt.imshow(img); plt.axis('off')
    plt.title("Fig.6 Global trends (screenshot)");
    plt.show()

if REGION_HTML and os.path.exists(REGION_HTML):
    html_to_png(REGION_HTML, "Fig7_region_trends.png", width=1600, height=1000, wait_sec=3)
    img = cv2.cvtColor(cv2.imread("Fig7_region_trends.png"), cv2.COLOR_BGR2RGB)
    plt.figure(figsize=(12,7)); plt.imshow(img); plt.axis('off')
    plt.title("Fig.7 Regional trends (screenshot)")
    plt.show()

# Fig.8: compute region-level summer area index and plot small multiples
import os, numpy as np, pandas as pd, matplotlib.pyplot as plt, math

# If automatic search failed above, set TS_CSV explicitly:
# TS_CSV = "/content/drive/MyDrive/your_project_folder/100Lake_area_Temperature_2000-2025.csv"

assert TS_CSV is not None and os.path.exists(TS_CSV), f"Time-series CSV not found: {TS_CSV}"
df = pd.read_csv(TS_CSV)

# Infer the 'year' column if missing (allowed: 'date' or 'Year')
if "year" not in df.columns:
    if "date" in df.columns:
        df["year"] = pd.to_datetime(df["date"]).dt.year
    elif "Year" in df.columns:
        df["year"] = df["Year"]
    else:
        raise ValueError("CSV must contain a 'year' or 'date' column.")

# Identify lake and region columns heuristically when names vary
def find_col(cands):
    for c in cands:
        if c in df.columns: return c
    for col in df.columns:
        l = col.lower()
        if any(k in l for k in cands): return col
    return None

lake_col   = find_col(["lake","lakename","name","lake","location"])
region_col = find_col(["region","zone","area","region","district"])
if lake_col is None:
    print("No lake column detected; using a global baseline across all records.")
if region_col is None:
    print("No region column detected; assigning all rows to 'All'.")
    region_col = "region"
    df[region_col] = "All"

# Identify area column; fall back to loose matching if necessary
area_candidates = ["area_index","area_m2","area_km2","area","Area","AREA"]
area_col = None
for c in area_candidates:
    if c in df.columns:
        area_col = c
        break
if area_col is None:
    for col in df.columns:
        l = col.lower()
        if "area" in l:
            area_col = col; break
if area_col is None:
    raise ValueError("CSV missing an area column (e.g. area_m2/area_km2/area).")

print(f"Detected columns: year={'year'}, region={region_col}, lake={lake_col}, area={area_col}")

# Compute normalised area index relative to an early-year baseline if missing
if "area_index" not in df.columns:
    df[area_col] = pd.to_numeric(df[area_col], errors="coerce")
    df = df.dropna(subset=[area_col, "year"])

    BASELINE_YEARS = {2000,2001,2002}

    def compute_baseline(g):
        # Prefer mean of years 2000–2002; otherwise use first three records by year
        cand = g[g["year"].isin(BASELINE_YEARS)][area_col]
        if cand.dropna().shape[0] >= 1:
            return float(cand.mean())
        return float(g.sort_values("year")[area_col].head(3).mean())

    if lake_col is not None:
        base_map = df.groupby(lake_col).apply(compute_baseline).to_dict()
        df["__baseline__"] = df[lake_col].map(base_map)
    else:
        all_base = compute_baseline(df.copy())
        df["__baseline__"] = all_base

    df = df[df["__baseline__"] > 0]
    df["area_index"] = df[area_col] / df["__baseline__"]
else:
    df["area_index"] = pd.to_numeric(df["area_index"], errors="coerce")
    df = df.dropna(subset=["area_index"])

# Aggregate by region and year; plot trend with optional linear fit
g = df.groupby([region_col,"year"], as_index=False)["area_index"].mean()

regions = sorted(g[region_col].unique().tolist())
ncol = min(3, len(regions)) if len(regions)>0 else 1
nrow = math.ceil(len(regions)/ncol) if len(regions)>0 else 1

fig, axes = plt.subplots(nrow, ncol, figsize=(5*ncol, 3.2*nrow), squeeze=False)

for i, reg in enumerate(regions):
    r, c = divmod(i, ncol)
    gi = g[g[region_col]==reg].sort_values("year")
    axes[r,c].plot(gi["year"], gi["area_index"], marker='o')
    if gi.shape[0] >= 2:
        x = gi["year"].values.astype(float); y = gi["area_index"].values.astype(float)
        k = np.polyfit(x, y, 1)
        axes[r,c].plot(x, k[0]*x + k[1], linestyle="--")
    axes[r,c].set_title(reg)
    axes[r,c].set_xlabel("Year"); axes[r,c].set_ylabel("Summer area index")
    axes[r,c].grid(True, linestyle="--", alpha=0.3)

# Hide any empty panels from grid layout
for j in range(len(regions), nrow*ncol):
    r, c = divmod(j, ncol); axes[r,c].axis("off")

plt.tight_layout()
plt.savefig("Fig8_region_small_multiples.png", dpi=300)
plt.show()
plt.close(fig)
print("Saved: Fig8_region_small_multiples.png")

# Fig.9: either permutation importance by region, or top-3 features per region (if provided)
out9 = None
if PERM_CSV and os.path.exists(PERM_CSV):
    dfp = pd.read_csv(PERM_CSV)
    region_col = next((c for c in dfp.columns if "region" in c.lower()), None)
    feat_col   = next((c for c in dfp.columns if "feature" in c.lower()), None)
    imp_col    = next((c for c in dfp.columns if "importance" in c.lower()), None)
    if region_col and feat_col and imp_col:
        gg = (dfp[[region_col, feat_col, imp_col]]
              .groupby(region_col, as_index=False)
              .apply(lambda x: x.sort_values(imp_col, ascending=False).head(5))
              .reset_index(drop=True))
        regs = gg[region_col].unique().tolist()
        ncol = min(3, len(regs)); nrow = (len(regs)+ncol-1)//ncol
        fig, axes = plt.subplots(nrow, ncol, figsize=(5*ncol, 3.5*nrow), squeeze=False)
        for i, reg in enumerate(regs):
            r,c = divmod(i, ncol)
            gi = gg[gg[region_col]==reg]
            axes[r,c].barh(gi[feat_col], gi[imp_col])
            axes[r,c].invert_yaxis()
            axes[r,c].set_title(reg); axes[r,c].set_xlabel("Permutation importance")
        for j in range(len(regs), nrow*ncol): r,c = divmod(j, ncol); axes[r,c].axis("off")
        plt.tight_layout(); out9 = "Fig9_permutation_importance.png"; plt.savefig(out9, dpi=300); plt.show(); plt.close(fig)
        print("Saved:", out9)

elif TOP3_CSV and os.path.exists(TOP3_CSV):
    dft = pd.read_csv(TOP3_CSV)
    region_col = next((c for c in dft.columns if "region" in c.lower()), None)
    feat_col   = next((c for c in dft.columns if "feature" in c.lower()), None)
    score_col  = next((c for c in dft.columns if "score" in c.lower()), None)
    if region_col and feat_col and score_col:
        regs = dft[region_col].unique().tolist()
        ncol = min(3, len(regs)); nrow = (len(regs)+ncol-1)//ncol
        fig, axes = plt.subplots(nrow, ncol, figsize=(5*ncol, 3.5*nrow), squeeze=False)
        for i, reg in enumerate(regs):
            r,c = divmod(i, ncol)
            gi = dft[dft[region_col]==reg]
            axes[r,c].barh(gi[feat_col], gi[score_col])
            axes[r,c].invert_yaxis()
            axes[r,c].set_title(reg); axes[r,c].set_xlabel("Top-3 score")
        for j in range(len(regs), nrow*ncol): r,c = divmod(j, ncol); axes[r,c].axis("off")
        plt.tight_layout(); out9 = "Fig9_top3_features.png"; plt.savefig(out9, dpi=300); plt.show(); plt.close(fig)
        print("Saved:", out9)
else:
    print("Importance/Top-3 CSV not found; Fig.9 skipped.")

from google.colab import files
outs = ["Fig2_sample_triptych.png","Fig3_qualitative_comparison.png",
        "Fig6_global_trends.png","Fig7_region_trends.png",
        "Fig8_region_small_multiples.png",
        "Fig9_permutation_importance.png","Fig9_top3_features.png"]
outs = [f for f in outs if os.path.exists(f)]

# Optionally copy outputs to Drive for archival
if SAVE_TO_DRIVE and MODE=="drive":
    import shutil, os
    out_dir = os.path.join(DRIVE_ROOT, "figs_out")
    os.makedirs(out_dir, exist_ok=True)
    for f in outs:
        shutil.copy2(f, os.path.join(out_dir, f))
    print("Copied to:", out_dir)

print("Select the images you want to download (multiple allowed):")
for f in outs:
    files.download(f)
