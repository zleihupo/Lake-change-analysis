# -*- coding: utf-8 -*-
"""check data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1S3qmcJpEMTebb31vNKS8Z6tIGa5qzZcb
"""

import pandas as pd
import numpy as np
import os, re
from google.colab import files
import matplotlib.pyplot as plt

#  Upload the CSV if it is not already present 
if not os.path.exists("/content/Lake_Area_and_Climate_2000_2025.csv"):
    print("Please upload Lake_Area_and_Climate_2000_2025.csv file")
    uploaded = files.upload()

# Read data
df = pd.read_csv("/content/Lake_Area_and_Climate_2000_2025.csv")

#  Clean 'area_m2' by extracting numeric tokens 
def to_numeric_area(val):
    m = re.search(r"-?\d+\.?\d*", str(val))
    if m:
        try:
            return float(m.group())
        except:
            return np.nan
    return np.nan

df['area_m2'] = df['area_m2'].apply(to_numeric_area)

#  Validate required columns 
required_cols = {'lake', 'hemisphere', 'year', 'area_m2'}
missing = required_cols - set(df.columns)
if missing:
    raise ValueError(f"Missing required columns: {missing}")

#  Aggregate records by lake–year
summary = (
    df.groupby(['lake','year'])
    .agg(
        n_records=('area_m2','count'),
        mean_area=('area_m2','mean')
    )
    .reset_index()
)

# Percentage change in mean area year-on-year within each lake
summary['pct_change'] = summary.groupby('lake')['mean_area'].pct_change(fill_method=None)

#  Simple quality flags 
summary['incomplete'] = summary['n_records'] < 3
summary['high_risk'] = (summary['incomplete']) & (summary['pct_change'].abs() > 0.5)

#  Year-level statistics 
year_stats = (
    summary.groupby('year')
    .agg(
        total_lakes=('lake','nunique'),
        high_risk_lakes=('high_risk','sum')
    )
    .reset_index()
)
year_stats['high_risk_ratio'] = year_stats['high_risk_lakes'] / year_stats['total_lakes']

#  Persist outputs to CSV
quality_path = "/content/quality_flags.csv"
summary.to_csv(quality_path, index=False)

suspect_path = "/content/suspect_years.csv"
summary[summary['high_risk']].to_csv(suspect_path, index=False)

year_stats_path = "/content/year_risk_stats.csv"
year_stats.to_csv(year_stats_path, index=False)

print("✅ Analysis completed")
print(f"Quality flags: {quality_path}")
print(f"Suspect years: {suspect_path}")
print(f"Yearly risk ratio: {year_stats_path}")

# Download results
files.download(quality_path)
files.download(suspect_path)
files.download(year_stats_path)

#  Plot summary chart 
plt.figure(figsize=(10,5))
years = year_stats['year']
ratios = year_stats['high_risk_ratio']

# Base line
plt.plot(years, ratios, marker='o', color='blue', label='High Risk Ratio')

# Highlight 2022–2025
mask_recent = (years >= 2022)
plt.plot(years[mask_recent], ratios[mask_recent], marker='o', color='red', linewidth=2, label='2022–2025')

# Label each point
for y, r in zip(years, ratios):
    plt.text(y, r + 0.01, f"{r:.2f}", ha='center', fontsize=8)

# Mark top 3
top3 = year_stats.sort_values('high_risk_ratio', ascending=False).head(3)
for _, row in top3.iterrows():
    plt.annotate(
        f"{int(row['year'])}\nRatio {row['high_risk_ratio']:.2f}",
        xy=(row['year'], row['high_risk_ratio']),
        xytext=(row['year'], row['high_risk_ratio'] + 0.15),
        arrowprops=dict(facecolor='black', shrink=0.05, width=1, headwidth=6),
        ha='center', fontsize=9, bbox=dict(boxstyle="round,pad=0.3", fc="yellow", alpha=0.5)
    )

plt.axhline(y=0.2, color='gray', linestyle='--', linewidth=1, label='0.2 Reference')
plt.title('Annual High-Risk Ratio (Records <3 and |Area change| >50%)', fontsize=14)
plt.xlabel('Year', fontsize=12)
plt.ylabel('High Risk Ratio', fontsize=12)
plt.ylim(0, 1.05)
plt.grid(True, linestyle='--', alpha=0.6)
plt.legend()
plt.show()
