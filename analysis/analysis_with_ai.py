# -*- coding: utf-8 -*-
"""analysis with AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z1pa9Wv263QLlGpKPgDHw1H1eNF-_XMQ
"""

from google.colab import drive
drive.mount('/content/drive')

# ========== 0) Dependency installation (if necessary) ==========
import sys, subprocess

def pip_install(pkgs):
    try:
        subprocess.check_call([sys.executable, "-m", "pip", "install", "-q"] + pkgs)
    except Exception as e:
        print("pip Installation failed:", e)

try:
    import statsmodels.api as sm  # noqa: F401
except:
    pip_install(["statsmodels"]) ; import statsmodels.api as sm

try:
    import shap  # noqa: F401
except:
    # Not mandatory, if it fails, the PDP direction will be automatically enabled
    try:
        pip_install(["shap"]) ; import shap
    except Exception:
        shap = None

# Excel Write Engine
try:
    import openpyxl  # noqa: F401
except:
    pip_install(["openpyxl"]) ; import openpyxl

try:
    import xlsxwriter  # noqa: F401
except:
    pip_install(["xlsxwriter"]) ; import xlsxwriter

# ========== 1)Import basic ==========
import os
import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from IPython.display import display
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.inspection import permutation_importance
from scipy.stats import spearmanr

# ========== 2) I/O Tools ==========
CONTENT_DIR = "/content"
CSV_DEFAULT = os.path.join(CONTENT_DIR, "100Lake_area_Temperature_2000-2025.csv")
XLSX_OUT = os.path.join(CONTENT_DIR, "lake_summer_AI_analysis.xlsx")


def show(df: pd.DataFrame, title: str = None, save_csv: bool = False, fname: str = None, head: int = 10):
    """General display functions: print shapes, display first few rows, optionally save to CSV."""
    if title:
        print("\n===", title, "===")
    print("shape:", df.shape)
    try:
        display(df.head(head))
    except Exception:
        print(df.head(head))
    if save_csv:
        if not fname:
            safe = (title or "table").replace(" ", "_").replace("/", "-")
            fname = os.path.join(CONTENT_DIR, f"{safe}.csv")
        df.to_csv(fname, index=False)
        print("saved:", fname)

# ========== 3) load data ==========
# Method A: Read directly from /content 
CSV_PATH = CSV_DEFAULT

if not os.path.exists(CSV_PATH):
    # Method B: If it does not exist, the upload window will pop up.
    try:
        from google.colab import files
        print("No CSV found in /content, please select local CSV upload...")
        uploaded = files.upload()
        if uploaded:
            # Get the first file
            CSV_PATH = list(uploaded.keys())[0]
    except Exception:
        # Method C: Optional Mount Drive Manually Specify Path
        print("To read from Google Drive, mount it first:")
        print("from google.colab import drive; drive.mount('/content/drive')")
        print("Then point CSV_PATH to your Drive path.")
        raise FileNotFoundError("CSV not found, please upload or modify CSV_PATH")

print("use data file:", CSV_PATH)

df = pd.read_csv(CSV_PATH)

# ========== 4) preprocessing ==========

def to_numeric(x):
    try:
        return float(str(x).replace(',', ''))
    except:
        return np.nan

# Cleaning area
if 'area_m2' not in df.columns:
    raise ValueError("The data is missing the 'area_m2' column")

df['area_m2'] = df['area_m2'].apply(to_numeric)

# Basic fields
for col in ['year', 'month']:
    if col not in df.columns:
        raise ValueError(f"Data is missing column '{col}'")

df['year'] = df['year'].astype(int)
df['month'] = df['month'].astype(int)
df['date'] = pd.to_datetime(dict(year=df['year'], month=df['month'], day=1))

# Keep only positive areas
df = df[df['area_m2'] > 0].copy()

# Hemisphere Summer Screening
if 'hemisphere' not in df.columns:
    raise ValueError("The data is missing the 'hemisphere' column (north/south)")


def is_summer(row):
    hemi = str(row['hemisphere']).lower()
    if hemi.startswith('north'):
        return row['month'] in [6, 7, 8]
    else:
        return row['month'] in [12, 1, 2]


df['is_summer'] = df.apply(is_summer, axis=1)
df = df[df['is_summer']].copy()

# Calculate the baseline area (summer average of 2000-2002). If there is no baseline area, fall back to the average of the first three summer records of the lake.
if 'lake' not in df.columns:
    raise ValueError("The data is missing the 'lake' column")

baseline = (
    df[(df['year'] >= 2000) & (df['year'] <= 2002)]
      .groupby('lake')['area_m2']
      .mean()
      .rename('baseline_area_m2')
)

first3 = (
    df.sort_values('date')
      .groupby('lake').head(3)
      .groupby('lake')['area_m2']
      .mean()
      .rename('fallback_baseline')
)

base = pd.concat([baseline, first3], axis=1)
base['baseline_area_m2'] = base['baseline_area_m2'].fillna(base['fallback_baseline'])
base = base[['baseline_area_m2']].dropna()
base = base[base['baseline_area_m2'] > 0]

# Merge and calculate area index
df = df.merge(base, left_on='lake', right_index=True, how='inner')
df['area_index'] = df['area_m2'] / df['baseline_area_m2']

# Aggregation: Summer lake-year granularity
required_feats = ['temp_C', 'precip_mm', 'pet_mm', 'snow_cover_pct', 'snow_depth_cm']
for c in required_feats + ['region', 'hemisphere']:
    if c not in df.columns:
        raise ValueError(f"Data is missing '{c}' column")

agg_cols = {
    'area_index': 'mean',
    'temp_C': 'mean',
    'precip_mm': 'mean',
    'pet_mm': 'mean',
    'snow_cover_pct': 'mean',
    'snow_depth_cm': 'mean',
}

summer_lake_year = (
    df.groupby(['region', 'lake', 'hemisphere', 'year'])
      .agg(**{k: (k, v) for k, v in agg_cols.items()})
      .reset_index()
)

show(summer_lake_year.head(20), "Summer lake-year aggregation preview", head=20)

# ========== 5) Regional trends (slope per decade) ==========
import statsmodels.api as sm


def slope_per_decade(d: pd.DataFrame, y: str):
    d = d.dropna(subset=[y, 'year']).copy()
    if d.empty or d['year'].nunique() < 4:
        return np.nan, np.nan
    X = sm.add_constant(d['year'])
    m = sm.OLS(d[y], X).fit()
    slope_decade = float(m.params['year'] * 10.0)
    p = float(m.pvalues['year'])
    return slope_decade, p


regions = sorted(summer_lake_year['region'].unique().tolist())
trend_rows = []
for reg in regions:
    d = (
        summer_lake_year[summer_lake_year['region'] == reg]
        .groupby('year')
        .agg(mean_temp=('temp_C', 'mean'),
             mean_area_index=('area_index', 'mean'))
        .reset_index()
    )
    sT, pT = slope_per_decade(d, 'mean_temp')
    sA, pA = slope_per_decade(d, 'mean_area_index')
    trend_rows.append({
        'region': reg,
        'temp_trend_C_per_decade': sT,
        'temp_trend_p': pT,
        'area_index_trend_per_decade': sA,
        'area_trend_p': pA,
    })

trend_table = pd.DataFrame(trend_rows).sort_values('region')
trend_show = trend_table.copy()
for c in ['temp_trend_C_per_decade', 'temp_trend_p', 'area_index_trend_per_decade', 'area_trend_p']:
    trend_show[c] = trend_show[c].round(3)
show(trend_show, "(Summer only) Regional-Decade Trend (Temperature & Area Index)", save_csv=True)

try:
    import shap
    SHAP_AVAILABLE = True
except:
    SHAP_AVAILABLE = False

# ========== 6) Machine Learning by Region (GBR)==========
features_raw = required_feats
perf_rows, imp_rows, direction_rows = [], [], []

for reg in regions:
    d = summer_lake_year[summer_lake_year['region'] == reg].dropna(subset=['area_index'] + features_raw).copy()
    if d['year'].nunique() < 6 or len(d) < 100:
        # Too few samples to skip
        continue

    # Encode only the hemisphere (drop_first=True -> the southern hemisphere is 1)
    dummies = pd.get_dummies(d[['hemisphere']], drop_first=True)
    X = pd.concat([d[features_raw], dummies], axis=1)
    y = d['area_index']

    # Time division：train<=2018, val 2019-2021, test>=2022
    train_mask = d['year'] <= 2018
    val_mask = (d['year'] >= 2019) & (d['year'] <= 2021)
    test_mask = d['year'] >= 2022

    # If splitting is not available, fallback to 7/3 time split
    if y[train_mask].empty or y[test_mask].empty:
        years_sorted = sorted(d['year'].unique())
        split_year = years_sorted[int(len(years_sorted) * 0.7)]
        train_mask = d['year'] <= split_year
        val_mask = (d['year'] > split_year) & (d['year'] <= split_year + 1)
        test_mask = d['year'] > split_year + 1

    X_train, y_train = X[train_mask], y[train_mask]
    X_val, y_val = X[val_mask], y[val_mask]
    X_test, y_test = X[test_mask], y[test_mask]

    # model
    model = GradientBoostingRegressor(random_state=42)
    model.fit(X_train, y_train)

    # evaluation
    def eval_metrics(X_, y_):
        pred = model.predict(X_)
        return {
            'MAE': float(mean_absolute_error(y_, pred)),
            'RMSE': float(np.sqrt(mean_squared_error(y_, pred))),
            'R2': float(r2_score(y_, pred)),
            'n': int(len(y_))
        }

    train_m = eval_metrics(X_train, y_train)
    test_m = eval_metrics(X_test, y_test)

    perf_rows.append({'region': reg, 'split': 'train', **train_m})
    perf_rows.append({'region': reg, 'split': 'test', **test_m})

    # Permutation Importance (Test Set)
    try:
        perm = permutation_importance(
            model, X_test, y_test,
            n_repeats=10, random_state=42, scoring='neg_mean_absolute_error'
        )
        for i, col in enumerate(X.columns):
            imp_rows.append({'region': reg, 'feature': col, 'perm_importance': float(perm.importances_mean[i])})
    except Exception as e:
        print(f"Permutation importance fail（{reg}）：", e)
        for col in X.columns:
            imp_rows.append({'region': reg, 'feature': col, 'perm_importance': np.nan})

    # Direction: Prioritize SHAP
    if SHAP_AVAILABLE:
        try:
            explainer = shap.Explainer(model)
            shap_vals = explainer(X_test)
            sv = shap_vals.values if hasattr(shap_vals, 'values') else shap_vals
            for j, col in enumerate(X.columns):
                mag = float(np.mean(np.abs(sv[:, j])))
                rho, _ = spearmanr(X_test[col], sv[:, j])
                direction_rows.append({'region': reg, 'feature': col, 'shap_abs_mean': mag, 'direction_rho': float(rho)})
        except Exception as e:
            print(f"SHAP fail（{reg}）：", e)
            SHAP_AVAILABLE = False  # Downgrade to PDP

    if not SHAP_AVAILABLE:
        # Directional regression: the slope of the PDP in the 20th-80th percentile range
        for col in X.columns:
            xs = X_test[col].values
            if len(xs) < 10 or np.std(xs) == 0:
                direction_rows.append({'region': reg, 'feature': col, 'pdp_slope': np.nan})
                continue
            q20, q80 = np.percentile(xs, [20, 80])
            grid = np.linspace(q20, q80, 10)
            X_ref_means = X_test.mean(axis=0)
            preds = []
            for g in grid:
                Xg = X_ref_means.to_frame().T.copy()
                Xg.index = [0]
                Xg[col] = g
                preds.append(float(model.predict(Xg)[0]))
            slope = (preds[-1] - preds[0]) / (q80 - q20) if (q80 - q20) != 0 else np.nan
            direction_rows.append({'region': reg, 'feature': col, 'pdp_slope': slope})

# Summary Table
perf_tbl = pd.DataFrame(perf_rows)
imp_tbl = pd.DataFrame(imp_rows)
dir_tbl = pd.DataFrame(direction_rows)

# Top-3 features（dismiss hemisphere dummy）

def summarize_top3(imp_df: pd.DataFrame) -> pd.DataFrame:
    rows = []
    for reg in sorted(imp_df['region'].unique()):
        imp_r = imp_df[imp_df['region'] == reg]
        if 'feature' not in imp_r.columns:
            continue
        imp_r = imp_r[~imp_r['feature'].str.contains('hemisphere_', na=False)].copy()
        imp_r = imp_r.sort_values('perm_importance', ascending=False).head(3)
        rows.append({
            'region': reg,
            'top1': imp_r['feature'].iloc[0] if len(imp_r) >= 1 else None,
            'top1_imp': float(imp_r['perm_importance'].iloc[0]) if len(imp_r) >= 1 else np.nan,
            'top2': imp_r['feature'].iloc[1] if len(imp_r) >= 2 else None,
            'top2_imp': float(imp_r['perm_importance'].iloc[1]) if len(imp_r) >= 2 else np.nan,
            'top3': imp_r['feature'].iloc[2] if len(imp_r) >= 3 else None,
            'top3_imp': float(imp_r['perm_importance'].iloc[2]) if len(imp_r) >= 3 else np.nan,
        })
    return pd.DataFrame(rows).sort_values('region')


# Incorporate directional information (SHAP rho first, PDP slope second)

def merge_direction(imp_df: pd.DataFrame, dir_df: pd.DataFrame) -> pd.DataFrame:
    out = []
    for _, r in imp_df.iterrows():
        reg, f = r['region'], r['feature']
        d = dir_df[(dir_df['region'] == reg) & (dir_df['feature'] == f)]
        if 'direction_rho' in d.columns and d['direction_rho'].notna().any():
            val = float(d['direction_rho'].dropna().mean())
            out.append({'region': reg, 'feature': f, 'dir_metric': val, 'dir_type': 'rho(shap)'})
        elif 'pdp_slope' in d.columns and d['pdp_slope'].notna().any():
            val = float(d['pdp_slope'].dropna().mean())
            out.append({'region': reg, 'feature': f, 'dir_metric': val, 'dir_type': 'pdp_slope'})
        else:
            out.append({'region': reg, 'feature': f, 'dir_metric': np.nan, 'dir_type': None})
    return pd.DataFrame(out)


dir_merge = merge_direction(imp_tbl, dir_tbl)

# Display main table
if not perf_tbl.empty:
    perf_pivot = perf_tbl.pivot_table(index='region', columns='split', values=['MAE', 'RMSE', 'R2', 'n'])
    perf_pivot.columns = ['_'.join(col).strip() for col in perf_pivot.columns.values]
    perf_pivot = perf_pivot.reset_index()
    for c in perf_pivot.columns:
        if c != 'region' and pd.api.types.is_numeric_dtype(perf_pivot[c]):
            perf_pivot[c] = perf_pivot[c].round(3)
    show(perf_pivot, "GBR Regional Performance (Summer)", save_csv=True)
else:
    perf_pivot = pd.DataFrame()
    print("Warning: perf_tbl is empty, possibly skipped due to insufficient samples for all regions.")

if not imp_tbl.empty:
    show(imp_tbl.sort_values(['region', 'perm_importance'], ascending=[True, False]).round(4),
         "Permutation Importance (By Region, Summer)", save_csv=True)

if not dir_merge.empty:
    show(dir_merge.sort_values(['region', 'feature']).round(4),
         "Feature Directional Index (rho(shap) or pdp_slope)", save_csv=True)

if not imp_tbl.empty:
    top3_tbl = summarize_top3(imp_tbl)
    show(top3_tbl.round(4), "Top 3 Important Features by Region (Summer)", save_csv=True)
else:
    top3_tbl = pd.DataFrame()

# ========== 7) 导出 Excel ==========
with pd.ExcelWriter(XLSX_OUT, engine='xlsxwriter') as writer:
    trend_show.to_excel(writer, sheet_name='Trends_per_decade', index=False)
    if not perf_pivot.empty:
        perf_pivot.to_excel(writer, sheet_name='Model_Perf', index=False)
    if not imp_tbl.empty:
        imp_tbl.to_excel(writer, sheet_name='Permutation_Importance', index=False)
    if not dir_merge.empty:
        dir_merge.to_excel(writer, sheet_name='Direction_Metric', index=False)
    if not top3_tbl.empty:
        top3_tbl.to_excel(writer, sheet_name='Top3', index=False)

print("\nExported to Excel:", XLSX_OUT)
print("Right-click in the file panel on the left of Colab to download, or use:")
print("from google.colab import files; files.download(r'" + XLSX_OUT + "')")

# ========== 8) Quick Summary/Review ==========
try:
    perf = pd.read_excel(XLSX_OUT, sheet_name='Model_Perf')
    if 'R2_test' in perf.columns:
        test_r2 = perf['R2_test'].dropna().values
        test_summary = {
            'min_R2_test': float(test_r2.min()) if len(test_r2) > 0 else None,
            'median_R2_test': float(pd.Series(test_r2).median()) if len(test_r2) > 0 else None,
            'max_R2_test': float(test_r2.max()) if len(test_r2) > 0 else None,
            'num_regions': int(len(test_r2))
        }
        print("\nTest Set R2 Overview:", test_summary)
except Exception as e:
    print("Skip the summary:", e)

!ls -lh /content
!zip -r /content/lake_analysis_outputs.zip /content/*.csv /content/*.xlsx
from google.colab import files
files.download('/content/lake_analysis_outputs.zip')
