# -*- coding: utf-8 -*-
"""analysis with AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z1pa9Wv263QLlGpKPgDHw1H1eNF-_XMQ
"""

from google.colab import drive
drive.mount('/content/drive')

# ========== 0) 依赖安装（如需要） ==========
import sys, subprocess

def pip_install(pkgs):
    try:
        subprocess.check_call([sys.executable, "-m", "pip", "install", "-q"] + pkgs)
    except Exception as e:
        print("pip 安装失败:", e)

# Colab 通常已自带 pandas/numpy/sklearn/matplotlib
# 但 statsmodels/shap/openpyxl/xlsxwriter 可能缺失
try:
    import statsmodels.api as sm  # noqa: F401
except:
    pip_install(["statsmodels"]) ; import statsmodels.api as sm

try:
    import shap  # noqa: F401
except:
    # 不强制，若失败将自动启用 PDP 方向
    try:
        pip_install(["shap"]) ; import shap
    except Exception:
        shap = None

# Excel 写入引擎
try:
    import openpyxl  # noqa: F401
except:
    pip_install(["openpyxl"]) ; import openpyxl

try:
    import xlsxwriter  # noqa: F401
except:
    pip_install(["xlsxwriter"]) ; import xlsxwriter

# ========== 1) 导入基础库 ==========
import os
import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from IPython.display import display
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.inspection import permutation_importance
from scipy.stats import spearmanr

# ========== 2) I/O 工具 ==========
CONTENT_DIR = "/content"
CSV_DEFAULT = os.path.join(CONTENT_DIR, "100Lake_area_Temperature_2000-2025.csv")
XLSX_OUT = os.path.join(CONTENT_DIR, "lake_summer_AI_analysis.xlsx")


def show(df: pd.DataFrame, title: str = None, save_csv: bool = False, fname: str = None, head: int = 10):
    """通用展示函数：打印形状、显示前几行，可选保存为 CSV。"""
    if title:
        print("\n===", title, "===")
    print("shape:", df.shape)
    try:
        display(df.head(head))
    except Exception:
        print(df.head(head))
    if save_csv:
        if not fname:
            safe = (title or "table").replace(" ", "_").replace("/", "-")
            fname = os.path.join(CONTENT_DIR, f"{safe}.csv")
        df.to_csv(fname, index=False)
        print("已保存:", fname)

# ========== 3) 读入数据 ==========
# 方式A：直接从 /content 读取（推荐将 CSV 放到左侧文件面板的 /content）
CSV_PATH = CSV_DEFAULT

if not os.path.exists(CSV_PATH):
    # 方式B：若不存在，弹出上传窗口
    try:
        from google.colab import files
        print("未在 /content 找到CSV，请选择本地CSV上传……")
        uploaded = files.upload()
        if uploaded:
            # 取第一个文件
            CSV_PATH = list(uploaded.keys())[0]
    except Exception:
        # 方式C：可选挂载 Drive 手动指定路径
        print("如需从 Google Drive 读取，请先挂载：")
        print("from google.colab import drive; drive.mount('/content/drive')")
        print("然后将 CSV_PATH 指到你的 Drive 路径即可。")
        raise FileNotFoundError("未找到 CSV，请上传或修改 CSV_PATH")

print("使用数据文件:", CSV_PATH)

df = pd.read_csv(CSV_PATH)

# ========== 4) 预处理（与原逻辑一致） ==========

def to_numeric(x):
    try:
        return float(str(x).replace(',', ''))
    except:
        return np.nan

# 清洗面积
if 'area_m2' not in df.columns:
    raise ValueError("数据缺少 'area_m2' 列")

df['area_m2'] = df['area_m2'].apply(to_numeric)

# 基本字段
for col in ['year', 'month']:
    if col not in df.columns:
        raise ValueError(f"数据缺少 '{col}' 列")

df['year'] = df['year'].astype(int)
df['month'] = df['month'].astype(int)
df['date'] = pd.to_datetime(dict(year=df['year'], month=df['month'], day=1))

# 仅保留正面积
df = df[df['area_m2'] > 0].copy()

# 半球夏季筛选
if 'hemisphere' not in df.columns:
    raise ValueError("数据缺少 'hemisphere' 列 (north/south)")


def is_summer(row):
    hemi = str(row['hemisphere']).lower()
    if hemi.startswith('north'):
        return row['month'] in [6, 7, 8]
    else:
        return row['month'] in [12, 1, 2]


df['is_summer'] = df.apply(is_summer, axis=1)
df = df[df['is_summer']].copy()

# 计算基线面积（2000-2002 夏季均值），无则回退为该湖首3条夏季记录均值
if 'lake' not in df.columns:
    raise ValueError("数据缺少 'lake' 列")

baseline = (
    df[(df['year'] >= 2000) & (df['year'] <= 2002)]
      .groupby('lake')['area_m2']
      .mean()
      .rename('baseline_area_m2')
)

first3 = (
    df.sort_values('date')
      .groupby('lake').head(3)
      .groupby('lake')['area_m2']
      .mean()
      .rename('fallback_baseline')
)

base = pd.concat([baseline, first3], axis=1)
base['baseline_area_m2'] = base['baseline_area_m2'].fillna(base['fallback_baseline'])
base = base[['baseline_area_m2']].dropna()
base = base[base['baseline_area_m2'] > 0]

# 合并并计算面积指数
df = df.merge(base, left_on='lake', right_index=True, how='inner')
df['area_index'] = df['area_m2'] / df['baseline_area_m2']

# 聚合：夏季 lake-year 粒度
required_feats = ['temp_C', 'precip_mm', 'pet_mm', 'snow_cover_pct', 'snow_depth_cm']
for c in required_feats + ['region', 'hemisphere']:
    if c not in df.columns:
        raise ValueError(f"数据缺少 '{c}' 列")

agg_cols = {
    'area_index': 'mean',
    'temp_C': 'mean',
    'precip_mm': 'mean',
    'pet_mm': 'mean',
    'snow_cover_pct': 'mean',
    'snow_depth_cm': 'mean',
}

summer_lake_year = (
    df.groupby(['region', 'lake', 'hemisphere', 'year'])
      .agg(**{k: (k, v) for k, v in agg_cols.items()})
      .reset_index()
)

show(summer_lake_year.head(20), "夏季 lake-year 聚合预览", head=20)

# ========== 5) 区域趋势（每10年斜率） ==========
import statsmodels.api as sm


def slope_per_decade(d: pd.DataFrame, y: str):
    d = d.dropna(subset=[y, 'year']).copy()
    if d.empty or d['year'].nunique() < 4:
        return np.nan, np.nan
    X = sm.add_constant(d['year'])
    m = sm.OLS(d[y], X).fit()
    slope_decade = float(m.params['year'] * 10.0)
    p = float(m.pvalues['year'])
    return slope_decade, p


regions = sorted(summer_lake_year['region'].unique().tolist())
trend_rows = []
for reg in regions:
    d = (
        summer_lake_year[summer_lake_year['region'] == reg]
        .groupby('year')
        .agg(mean_temp=('temp_C', 'mean'),
             mean_area_index=('area_index', 'mean'))
        .reset_index()
    )
    sT, pT = slope_per_decade(d, 'mean_temp')
    sA, pA = slope_per_decade(d, 'mean_area_index')
    trend_rows.append({
        'region': reg,
        'temp_trend_C_per_decade': sT,
        'temp_trend_p': pT,
        'area_index_trend_per_decade': sA,
        'area_trend_p': pA,
    })

trend_table = pd.DataFrame(trend_rows).sort_values('region')
trend_show = trend_table.copy()
for c in ['temp_trend_C_per_decade', 'temp_trend_p', 'area_index_trend_per_decade', 'area_trend_p']:
    trend_show[c] = trend_show[c].round(3)
show(trend_show, "(Summer only) 区域-十年趋势（温度 & 面积指数）", save_csv=True)

try:
    import shap
    SHAP_AVAILABLE = True
except:
    SHAP_AVAILABLE = False

# ========== 6) 各区域机器学习（GBR） ==========
features_raw = required_feats
perf_rows, imp_rows, direction_rows = [], [], []

for reg in regions:
    d = summer_lake_year[summer_lake_year['region'] == reg].dropna(subset=['area_index'] + features_raw).copy()
    if d['year'].nunique() < 6 or len(d) < 100:
        # 样本太少跳过
        continue

    # 仅编码 hemisphere（drop_first=True -> 南半球为1）
    dummies = pd.get_dummies(d[['hemisphere']], drop_first=True)
    X = pd.concat([d[features_raw], dummies], axis=1)
    y = d['area_index']

    # 时间切分：train<=2018, val 2019-2021, test>=2022
    train_mask = d['year'] <= 2018
    val_mask = (d['year'] >= 2019) & (d['year'] <= 2021)
    test_mask = d['year'] >= 2022

    # 若切分不可用，按7/3时间分割回退
    if y[train_mask].empty or y[test_mask].empty:
        years_sorted = sorted(d['year'].unique())
        split_year = years_sorted[int(len(years_sorted) * 0.7)]
        train_mask = d['year'] <= split_year
        val_mask = (d['year'] > split_year) & (d['year'] <= split_year + 1)
        test_mask = d['year'] > split_year + 1

    X_train, y_train = X[train_mask], y[train_mask]
    X_val, y_val = X[val_mask], y[val_mask]
    X_test, y_test = X[test_mask], y[test_mask]

    # 模型
    model = GradientBoostingRegressor(random_state=42)
    model.fit(X_train, y_train)

    # 评估
    def eval_metrics(X_, y_):
        pred = model.predict(X_)
        return {
            'MAE': float(mean_absolute_error(y_, pred)),
            'RMSE': float(np.sqrt(mean_squared_error(y_, pred))),
            'R2': float(r2_score(y_, pred)),
            'n': int(len(y_))
        }

    train_m = eval_metrics(X_train, y_train)
    test_m = eval_metrics(X_test, y_test)

    perf_rows.append({'region': reg, 'split': 'train', **train_m})
    perf_rows.append({'region': reg, 'split': 'test', **test_m})

    # 置换重要度（测试集）
    try:
        perm = permutation_importance(
            model, X_test, y_test,
            n_repeats=10, random_state=42, scoring='neg_mean_absolute_error'
        )
        for i, col in enumerate(X.columns):
            imp_rows.append({'region': reg, 'feature': col, 'perm_importance': float(perm.importances_mean[i])})
    except Exception as e:
        print(f"Permutation importance 失败（{reg}）：", e)
        for col in X.columns:
            imp_rows.append({'region': reg, 'feature': col, 'perm_importance': np.nan})

    # 方向：优先 SHAP
    if SHAP_AVAILABLE:
        try:
            explainer = shap.Explainer(model)
            shap_vals = explainer(X_test)
            sv = shap_vals.values if hasattr(shap_vals, 'values') else shap_vals
            for j, col in enumerate(X.columns):
                mag = float(np.mean(np.abs(sv[:, j])))
                rho, _ = spearmanr(X_test[col], sv[:, j])
                direction_rows.append({'region': reg, 'feature': col, 'shap_abs_mean': mag, 'direction_rho': float(rho)})
        except Exception as e:
            print(f"SHAP 失败（{reg}）：", e)
            SHAP_AVAILABLE = False  # 降级到 PDP

    if not SHAP_AVAILABLE:
        # 方向回退：PDP 在 20-80 分位区间的斜率
        for col in X.columns:
            xs = X_test[col].values
            if len(xs) < 10 or np.std(xs) == 0:
                direction_rows.append({'region': reg, 'feature': col, 'pdp_slope': np.nan})
                continue
            q20, q80 = np.percentile(xs, [20, 80])
            grid = np.linspace(q20, q80, 10)
            X_ref_means = X_test.mean(axis=0)
            preds = []
            for g in grid:
                Xg = X_ref_means.to_frame().T.copy()
                Xg.index = [0]
                Xg[col] = g
                preds.append(float(model.predict(Xg)[0]))
            slope = (preds[-1] - preds[0]) / (q80 - q20) if (q80 - q20) != 0 else np.nan
            direction_rows.append({'region': reg, 'feature': col, 'pdp_slope': slope})

# 汇总表
perf_tbl = pd.DataFrame(perf_rows)
imp_tbl = pd.DataFrame(imp_rows)
dir_tbl = pd.DataFrame(direction_rows)

# Top-3 features（忽略 hemisphere dummy）

def summarize_top3(imp_df: pd.DataFrame) -> pd.DataFrame:
    rows = []
    for reg in sorted(imp_df['region'].unique()):
        imp_r = imp_df[imp_df['region'] == reg]
        if 'feature' not in imp_r.columns:
            continue
        imp_r = imp_r[~imp_r['feature'].str.contains('hemisphere_', na=False)].copy()
        imp_r = imp_r.sort_values('perm_importance', ascending=False).head(3)
        rows.append({
            'region': reg,
            'top1': imp_r['feature'].iloc[0] if len(imp_r) >= 1 else None,
            'top1_imp': float(imp_r['perm_importance'].iloc[0]) if len(imp_r) >= 1 else np.nan,
            'top2': imp_r['feature'].iloc[1] if len(imp_r) >= 2 else None,
            'top2_imp': float(imp_r['perm_importance'].iloc[1]) if len(imp_r) >= 2 else np.nan,
            'top3': imp_r['feature'].iloc[2] if len(imp_r) >= 3 else None,
            'top3_imp': float(imp_r['perm_importance'].iloc[2]) if len(imp_r) >= 3 else np.nan,
        })
    return pd.DataFrame(rows).sort_values('region')


# 合并方向信息（优先 SHAP rho，其次 PDP slope）

def merge_direction(imp_df: pd.DataFrame, dir_df: pd.DataFrame) -> pd.DataFrame:
    out = []
    for _, r in imp_df.iterrows():
        reg, f = r['region'], r['feature']
        d = dir_df[(dir_df['region'] == reg) & (dir_df['feature'] == f)]
        if 'direction_rho' in d.columns and d['direction_rho'].notna().any():
            val = float(d['direction_rho'].dropna().mean())
            out.append({'region': reg, 'feature': f, 'dir_metric': val, 'dir_type': 'rho(shap)'})
        elif 'pdp_slope' in d.columns and d['pdp_slope'].notna().any():
            val = float(d['pdp_slope'].dropna().mean())
            out.append({'region': reg, 'feature': f, 'dir_metric': val, 'dir_type': 'pdp_slope'})
        else:
            out.append({'region': reg, 'feature': f, 'dir_metric': np.nan, 'dir_type': None})
    return pd.DataFrame(out)


dir_merge = merge_direction(imp_tbl, dir_tbl)

# 展示主要表格
if not perf_tbl.empty:
    perf_pivot = perf_tbl.pivot_table(index='region', columns='split', values=['MAE', 'RMSE', 'R2', 'n'])
    perf_pivot.columns = ['_'.join(col).strip() for col in perf_pivot.columns.values]
    perf_pivot = perf_pivot.reset_index()
    for c in perf_pivot.columns:
        if c != 'region' and pd.api.types.is_numeric_dtype(perf_pivot[c]):
            perf_pivot[c] = perf_pivot[c].round(3)
    show(perf_pivot, "GBR 各区域性能（夏季）", save_csv=True)
else:
    perf_pivot = pd.DataFrame()
    print("警告：perf_tbl 为空，可能因为所有区域样本不足被跳过。")

if not imp_tbl.empty:
    show(imp_tbl.sort_values(['region', 'perm_importance'], ascending=[True, False]).round(4),
         "Permutation importance（按区域，夏季）", save_csv=True)

if not dir_merge.empty:
    show(dir_merge.sort_values(['region', 'feature']).round(4),
         "特征方向指标（rho(shap) 或 pdp_slope）", save_csv=True)

if not imp_tbl.empty:
    top3_tbl = summarize_top3(imp_tbl)
    show(top3_tbl.round(4), "各区域 Top-3 重要特征（夏季）", save_csv=True)
else:
    top3_tbl = pd.DataFrame()

# ========== 7) 导出 Excel ==========
with pd.ExcelWriter(XLSX_OUT, engine='xlsxwriter') as writer:
    trend_show.to_excel(writer, sheet_name='Trends_per_decade', index=False)
    if not perf_pivot.empty:
        perf_pivot.to_excel(writer, sheet_name='Model_Perf', index=False)
    if not imp_tbl.empty:
        imp_tbl.to_excel(writer, sheet_name='Permutation_Importance', index=False)
    if not dir_merge.empty:
        dir_merge.to_excel(writer, sheet_name='Direction_Metric', index=False)
    if not top3_tbl.empty:
        top3_tbl.to_excel(writer, sheet_name='Top3', index=False)

print("\n已导出 Excel:", XLSX_OUT)
print("在 Colab 左侧文件面板中右键下载，或使用：")
print("from google.colab import files; files.download(r'" + XLSX_OUT + "')")

# ========== 8) （可选）快速摘要/检查 ==========
try:
    perf = pd.read_excel(XLSX_OUT, sheet_name='Model_Perf')
    if 'R2_test' in perf.columns:
        test_r2 = perf['R2_test'].dropna().values
        test_summary = {
            'min_R2_test': float(test_r2.min()) if len(test_r2) > 0 else None,
            'median_R2_test': float(pd.Series(test_r2).median()) if len(test_r2) > 0 else None,
            'max_R2_test': float(test_r2.max()) if len(test_r2) > 0 else None,
            'num_regions': int(len(test_r2))
        }
        print("\n测试集 R2 概览:", test_summary)
except Exception as e:
    print("摘要环节跳过：", e)

!ls -lh /content
!zip -r /content/lake_analysis_outputs.zip /content/*.csv /content/*.xlsx
from google.colab import files
files.download('/content/lake_analysis_outputs.zip')