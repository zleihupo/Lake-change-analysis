# -*- coding: utf-8 -*-
"""analysis with AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z1pa9Wv263QLlGpKPgDHw1H1eNF-_XMQ
"""

from google.colab import drive
drive.mount('/content/drive')

#  Dependency installation (run if a package is missing) 
import sys, subprocess

def pip_install(pkgs):
    try:
        subprocess.check_call([sys.executable, "-m", "pip", "install", "-q"] + pkgs)
    except Exception as e:
        print("pip Installation failed:", e)

try:
    import statsmodels.api as sm  # noqa: F401
except:
    pip_install(["statsmodels"]) ; import statsmodels.api as sm

try:
    import shap  # noqa: F401
except:
    # Optional dependency; if unavailable the PDP-based direction is used instead
    try:
        pip_install(["shap"]) ; import shap
    except Exception:
        shap = None

# Excel backends for writing .xlsx files
try:
    import openpyxl  # noqa: F401
except:
    pip_install(["openpyxl"]) ; import openpyxl

try:
    import xlsxwriter  # noqa: F401
except:
    pip_install(["xlsxwriter"]) ; import xlsxwriter

#  Import basic libraries 
import os
import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from IPython.display import display
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.inspection import permutation_importance
from scipy.stats import spearmanr

#  I/O paths and defaults 
CONTENT_DIR = "/content"
CSV_DEFAULT = os.path.join(CONTENT_DIR, "100Lake_area_Temperature_2000-2025.csv")
XLSX_OUT = os.path.join(CONTENT_DIR, "lake_summer_AI_analysis.xlsx")


def show(df: pd.DataFrame, title: str = None, save_csv: bool = False, fname: str = None, head: int = 10):
    """Compact table preview and optional CSV export."""
    if title:
        print("\n===", title, "===")
    print("shape:", df.shape)
    try:
        display(df.head(head))
    except Exception:
        print(df.head(head))
    if save_csv:
        if not fname:
            safe = (title or "table").replace(" ", "_").replace("/", "-")
            fname = os.path.join(CONTENT_DIR, f"{safe}.csv")
        df.to_csv(fname, index=False)
        print("saved:", fname)

#  Load data 
# Method A: attempt to read from /content 
CSV_PATH = CSV_DEFAULT

if not os.path.exists(CSV_PATH):
    # Method B: fall back to manual upload
    try:
        from google.colab import files
        print("No CSV found in /content, please select local CSV upload...")
        uploaded = files.upload()
        if uploaded:
            # Use the first uploaded file
            CSV_PATH = list(uploaded.keys())[0]
    except Exception:
        # Method C: instruct user to mount Drive and set path
        print("To read from Google Drive, mount it first:")
        print("from google.colab import drive; drive.mount('/content/drive')")
        print("Then point CSV_PATH to your Drive path.")
        raise FileNotFoundError("CSV not found, please upload or modify CSV_PATH")

print("use data file:", CSV_PATH)

df = pd.read_csv(CSV_PATH)

#  Pre-processing

def to_numeric(x):
    try:
        return float(str(x).replace(',', ''))
    except:
        return np.nan

# Validate and clean area column
if 'area_m2' not in df.columns:
    raise ValueError("Missing 'area_m2' column in the dataset")

df['area_m2'] = df['area_m2'].apply(to_numeric)

# Validate and cast year
if 'year' not in df.columns:
    raise ValueError("Missing 'year' column in the dataset")
df['year'] = df['year'].astype(int)

#  Keep summer flag even if month is absent; fall back to all-summer
if 'month' not in df.columns:
    print(" 'month' column not found; skipping 'date' and 'is_summer' processing")
    df['is_summer'] = True  # default to summer
else:
    df['month'] = df['month'].astype(int)
    df['date'] = pd.to_datetime(dict(year=df['year'], month=df['month'], day=1))

    def is_summer(row):
        hemi = str(row['hemisphere']).lower()
        if hemi.startswith('north'):
            return row['month'] in [6, 7, 8]
        else:
            return row['month'] in [12, 1, 2]

    df['is_summer'] = df.apply(is_summer, axis=1)

# Filter invalid area values
df = df[df['area_m2'] > 0].copy()

# Ensure required identifiers exist
if 'lake' not in df.columns:
    raise ValueError("Missing 'lake' column in the dataset")
if 'hemisphere' not in df.columns:
    raise ValueError("Missing 'hemisphere' column in the dataset")

#  Retain summer observations (logical guardrail even if already all-summer)
df = df[df['is_summer']].copy()

# Compute per-lake baseline (2000–2002) with fallback to the first 3 records
baseline = (
    df[(df['year'] >= 2000) & (df['year'] <= 2002)]
      .groupby('lake')['area_m2']
      .mean()
      .rename('baseline_area_m2')
)

first3 = (
    df.sort_values('year')
      .groupby('lake').head(3)
      .groupby('lake')['area_m2']
      .mean()
      .rename('fallback_baseline')
)

base = pd.concat([baseline, first3], axis=1)
base['baseline_area_m2'] = base['baseline_area_m2'].fillna(base['fallback_baseline'])
base = base[['baseline_area_m2']].dropna()
base = base[base['baseline_area_m2'] > 0]

# Merge baseline and compute area index
df = df.merge(base, left_on='lake', right_index=True, how='inner')
df['area_index'] = df['area_m2'] / df['baseline_area_m2']

# Aggregate to lake–year within regions (summer means)
required_feats = ['temp_C', 'precip_mm', 'pet_mm', 'snow_cover_pct', 'snow_depth_cm']
for c in required_feats + ['region', 'hemisphere']:
    if c not in df.columns:
        raise ValueError(f"Missing required column: '{c}'")

agg_cols = {
    'area_index': 'mean',
    'temp_C': 'mean',
    'precip_mm': 'mean',
    'pet_mm': 'mean',
    'snow_cover_pct': 'mean',
    'snow_depth_cm': 'mean',
}

summer_lake_year = (
    df.groupby(['region', 'lake', 'hemisphere', 'year'])
      .agg(**{k: (k, v) for k, v in agg_cols.items()})
      .reset_index()
)

show(summer_lake_year.head(20), "Preview of summer lake-year aggregation", head=20)


#  Regional trends (slope per decade) 
import statsmodels.api as sm


def slope_per_decade(d: pd.DataFrame, y: str):
    d = d.dropna(subset=[y, 'year']).copy()
    if d.empty or d['year'].nunique() < 4:
        return np.nan, np.nan
    X = sm.add_constant(d['year'])
    m = sm.OLS(d[y], X).fit()
    slope_decade = float(m.params['year'] * 10.0)
    p = float(m.pvalues['year'])
    return slope_decade, p


regions = sorted(summer_lake_year['region'].unique().tolist())
trend_rows = []
for reg in regions:
    d = (
        summer_lake_year[summer_lake_year['region'] == reg]
        .groupby('year')
        .agg(mean_temp=('temp_C', 'mean'),
             mean_area_index=('area_index', 'mean'))
        .reset_index()
    )
    sT, pT = slope_per_decade(d, 'mean_temp')
    sA, pA = slope_per_decade(d, 'mean_area_index')
    trend_rows.append({
        'region': reg,
        'temp_trend_C_per_decade': sT,
        'temp_trend_p': pT,
        'area_index_trend_per_decade': sA,
        'area_trend_p': pA,
    })

trend_table = pd.DataFrame(trend_rows).sort_values('region')
trend_show = trend_table.copy()
for c in ['temp_trend_C_per_decade', 'temp_trend_p', 'area_index_trend_per_decade', 'area_trend_p']:
    trend_show[c] = trend_show[c].round(3)
show(trend_show, "(Summer only) Regional-Decade Trend (Temperature & Area Index)", save_csv=True)

try:
    import shap
    SHAP_AVAILABLE = True
except:
    SHAP_AVAILABLE = False

#6) Machine learning by region (GBR)
features_raw = required_feats
perf_rows, imp_rows, direction_rows = [], [], []

for reg in regions:
    d = summer_lake_year[summer_lake_year['region'] == reg].dropna(subset=['area_index'] + features_raw).copy()
    if d['year'].nunique() < 6 or len(d) < 100:
        # Too few samples; skip this region
        continue

    # Encode the hemisphere only (drop_first=True → southern hemisphere encoded as 1)
    dummies = pd.get_dummies(d[['hemisphere']], drop_first=True)
    X = pd.concat([d[features_raw], dummies], axis=1)
    y = d['area_index']

    # Time split: train ≤ 2018; val 2019–2021; test ≥ 2022
    train_mask = d['year'] <= 2018
    val_mask = (d['year'] >= 2019) & (d['year'] <= 2021)
    test_mask = d['year'] >= 2022

    # If unavailable, fall back to a 70/30 time split with a narrow validation window
    if y[train_mask].empty or y[test_mask].empty:
        years_sorted = sorted(d['year'].unique())
        split_year = years_sorted[int(len(years_sorted) * 0.7)]
        train_mask = d['year'] <= split_year
        val_mask = (d['year'] > split_year) & (d['year'] <= split_year + 1)
        test_mask = d['year'] > split_year + 1

    X_train, y_train = X[train_mask], y[train_mask]
    X_val, y_val = X[val_mask], y[val_mask]
    X_test, y_test = X[test_mask], y[test_mask]

    # model
    model = GradientBoostingRegressor(random_state=42)
    model.fit(X_train, y_train)

    # evaluation
    def eval_metrics(X_, y_):
        pred = model.predict(X_)
        return {
            'MAE': float(mean_absolute_error(y_, pred)),
            'RMSE': float(np.sqrt(mean_squared_error(y_, pred))),
            'R2': float(r2_score(y_, pred)),
            'n': int(len(y_))
        }

    train_m = eval_metrics(X_train, y_train)
    test_m = eval_metrics(X_test, y_test)

    perf_rows.append({'region': reg, 'split': 'train', **train_m})
    perf_rows.append({'region': reg, 'split': 'test', **test_m})

    # Permutation importance on the test split
    try:
        perm = permutation_importance(
            model, X_test, y_test,
            n_repeats=10, random_state=42, scoring='neg_mean_absolute_error'
        )
        for i, col in enumerate(X.columns):
            imp_rows.append({'region': reg, 'feature': col, 'perm_importance': float(perm.importances_mean[i])})
    except Exception as e:
        print(f"Permutation importance fail（{reg}）：", e)
        for col in X.columns:
            imp_rows.append({'region': reg, 'feature': col, 'perm_importance': np.nan})

    # Direction: prefer SHAP when available; otherwise use PDP slope
    if SHAP_AVAILABLE:
        try:
            explainer = shap.Explainer(model)
            shap_vals = explainer(X_test)
            sv = shap_vals.values if hasattr(shap_vals, 'values') else shap_vals
            for j, col in enumerate(X.columns):
                mag = float(np.mean(np.abs(sv[:, j])))
                rho, _ = spearmanr(X_test[col], sv[:, j])
                direction_rows.append({'region': reg, 'feature': col, 'shap_abs_mean': mag, 'direction_rho': float(rho)})
        except Exception as e:
            print(f"SHAP fail（{reg}）：", e)
            SHAP_AVAILABLE = False  # revert to PDP

    if not SHAP_AVAILABLE:
        # PDP-based direction: slope between 20th–80th percentiles
        for col in X.columns:
            xs = X_test[col].values
            if len(xs) < 10 or np.std(xs) == 0:
                direction_rows.append({'region': reg, 'feature': col, 'pdp_slope': np.nan})
                continue
            q20, q80 = np.percentile(xs, [20, 80])
            grid = np.linspace(q20, q80, 10)
            X_ref_means = X_test.mean(axis=0)
            preds = []
            for g in grid:
                Xg = X_ref_means.to_frame().T.copy()
                Xg.index = [0]
                Xg[col] = g
                preds.append(float(model.predict(Xg)[0]))
            slope = (preds[-1] - preds[0]) / (q80 - q20) if (q80 - q20) != 0 else np.nan
            direction_rows.append({'region': reg, 'feature': col, 'pdp_slope': slope})

# Summary tables
perf_tbl = pd.DataFrame(perf_rows)
imp_tbl = pd.DataFrame(imp_rows)
dir_tbl = pd.DataFrame(direction_rows)

# Top-3 features (drop hemisphere dummy variables)

def summarize_top3(imp_df: pd.DataFrame) -> pd.DataFrame:
    rows = []
    for reg in sorted(imp_df['region'].unique()):
        imp_r = imp_df[imp_df['region'] == reg]
        if 'feature' not in imp_r.columns:
            continue
        imp_r = imp_r[~imp_r['feature'].str.contains('hemisphere_', na=False)].copy()
        imp_r = imp_r.sort_values('perm_importance', ascending=False).head(3)
        rows.append({
            'region': reg,
            'top1': imp_r['feature'].iloc[0] if len(imp_r) >= 1 else None,
            'top1_imp': float(imp_r['perm_importance'].iloc[0]) if len(imp_r) >= 1 else np.nan,
            'top2': imp_r['feature'].iloc[1] if len(imp_r) >= 2 else None,
            'top2_imp': float(imp_r['perm_importance'].iloc[1]) if len(imp_r) >= 2 else np.nan,
            'top3': imp_r['feature'].iloc[2] if len(imp_r) >= 3 else None,
            'top3_imp': float(imp_r['perm_importance'].iloc[2]) if len(imp_r) >= 3 else np.nan,
        })
    return pd.DataFrame(rows).sort_values('region')


# Incorporate directional metrics (SHAP rho preferred; PDP slope otherwise)

def merge_direction(imp_df: pd.DataFrame, dir_df: pd.DataFrame) -> pd.DataFrame:
    out = []
    for _, r in imp_df.iterrows():
        reg, f = r['region'], r['feature']
        d = dir_df[(dir_df['region'] == reg) & (dir_df['feature'] == f)]
        if 'direction_rho' in d.columns and d['direction_rho'].notna().any():
            val = float(d['direction_rho'].dropna().mean())
            out.append({'region': reg, 'feature': f, 'dir_metric': val, 'dir_type': 'rho(shap)'})
        elif 'pdp_slope' in d.columns and d['pdp_slope'].notna().any():
            val = float(d['pdp_slope'].dropna().mean())
            out.append({'region': reg, 'feature': f, 'dir_metric': val, 'dir_type': 'pdp_slope'})
        else:
            out.append({'region': reg, 'feature': f, 'dir_metric': np.nan, 'dir_type': None})
    return pd.DataFrame(out)


dir_merge = merge_direction(imp_tbl, dir_tbl)

# Display key outputs
if not perf_tbl.empty:
    perf_pivot = perf_tbl.pivot_table(index='region', columns='split', values=['MAE', 'RMSE', 'R2', 'n'])
    perf_pivot.columns = ['_'.join(col).strip() for col in perf_pivot.columns.values]
    perf_pivot = perf_pivot.reset_index()
    for c in perf_pivot.columns:
        if c != 'region' and pd.api.types.is_numeric_dtype(perf_pivot[c]):
            perf_pivot[c] = perf_pivot[c].round(3)
    show(perf_pivot, "GBR Regional Performance (Summer)", save_csv=True)
else:
    perf_pivot = pd.DataFrame()
    print("Warning: perf_tbl is empty, possibly skipped due to insufficient samples for all regions.")

if not imp_tbl.empty:
    show(imp_tbl.sort_values(['region', 'perm_importance'], ascending=[True, False]).round(4),
         "Permutation Importance (By Region, Summer)", save_csv=True)

if not dir_merge.empty:
    show(dir_merge.sort_values(['region', 'feature']).round(4),
         "Feature Directional Index (rho(shap) or pdp_slope)", save_csv=True)

if not imp_tbl.empty:
    top3_tbl = summarize_top3(imp_tbl)
    show(top3_tbl.round(4), "Top 3 Important Features by Region (Summer)", save_csv=True)
else:
    top3_tbl = pd.DataFrame()

#  Export Excel workbook 
with pd.ExcelWriter(XLSX_OUT, engine='xlsxwriter') as writer:
    trend_show.to_excel(writer, sheet_name='Trends_per_decade', index=False)
    if not perf_pivot.empty:
        perf_pivot.to_excel(writer, sheet_name='Model_Perf', index=False)
    if not imp_tbl.empty:
        imp_tbl.to_excel(writer, sheet_name='Permutation_Importance', index=False)
    if not dir_merge.empty:
        dir_merge.to_excel(writer, sheet_name='Direction_Metric', index=False)
    if not top3_tbl.empty:
        top3_tbl.to_excel(writer, sheet_name='Top3', index=False)

print("\nExported to Excel:", XLSX_OUT)
print("Right-click in the file panel on the left of Colab to download, or use:")
print("from google.colab import files; files.download(r'" + XLSX_OUT + "')")

#  Quick summary (optional) 
try:
    perf = pd.read_excel(XLSX_OUT, sheet_name='Model_Perf')
    if 'R2_test' in perf.columns:
        test_r2 = perf['R2_test'].dropna().values
        test_summary = {
            'min_R2_test': float(test_r2.min()) if len(test_r2) > 0 else None,
            'median_R2_test': float(pd.Series(test_r2).median()) if len(test_r2) > 0 else None,
            'max_R2_test': float(test_r2.max()) if len(test_r2) > 0 else None,
            'num_regions': int(len(test_r2))
        }
        print("\nTest Set R2 Overview:", test_summary)
except Exception as e:
    print("Skip the summary:", e)

!ls -lh /content
!zip -r /content/lake_analysis_outputs.zip /content/*.csv /content/*.xlsx
from google.colab import files
files.download('/content/lake_analysis_outputs.zip')
